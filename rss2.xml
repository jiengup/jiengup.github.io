<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GuntherX&#39;s</title>
    <link>https://jiengup.github.io/</link>
    
    <image>
      <url>https://jiengup.github.io/favicon.png</url>
      <title>GuntherX&#39;s</title>
      <link>https://jiengup.github.io/</link>
    </image>
    
    <atom:link href="https://jiengup.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>This is a tiny blog about my experiences and thoughts.</description>
    <pubDate>Wed, 23 Oct 2024 16:10:53 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>如何给开源项目做贡献</title>
      <link>https://jiengup.github.io/2023/12/11/17/37/</link>
      <guid>https://jiengup.github.io/2023/12/11/17/37/</guid>
      <pubDate>Mon, 11 Dec 2023 09:37:38 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;如何寻找一个合适的开源项目&quot;&gt;1. 如何寻找一个合适的开源项目&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;可以选择知名的开源软件基金会的孵化项目，比如Apache基金会、Linux基金会、CNCF等寻找一个不那么成熟，但是又形成了代码规范、协作流程规范的入门项目
&lt;ul&gt;
&lt;l</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="如何寻找一个合适的开源项目">1. 如何寻找一个合适的开源项目</h2><ul><li>可以选择知名的开源软件基金会的孵化项目，比如Apache基金会、Linux基金会、CNCF等寻找一个不那么成熟，但是又形成了代码规范、协作流程规范的入门项目<ul><li><a href="https://www.cncf.io/sandbox-projects/">CNCF沙箱项目</a></li><li><a href="https://www.cncf.io/projects/">CNCF孵化项目(列表包括毕业项目)</a></li><li><a href="https://projects.apache.org/projects.html">Apache项目(孵化期项目名字中带 Incubating)</a></li></ul></li></ul><h2 id="寻找贡献点">2. 寻找贡献点</h2><ol type="1"><li>在Issues里面，可以找到带有<strong>good firestissue</strong>标签标记的issue</li><li>找到合适的issue之后，在下面留言，等待项目管理员回复，将该issue分配给你</li><li>在正式开工之前，需要过一遍该项目的<strong>CONTRIBUTING.md</strong>文档，明晰该项目的贡献规范</li></ol><h2 id="开工">3. 开工</h2><ol type="1"><li>Fork项目仓库</li><li>将项目仓库克隆到本地</li><li>配置git remote，注意gitremote的pull配置为云端仓库，而push则配置为garbege，注意<strong>本地的代码变更永远只提交到origin，然后通过origin 以提交 Pull Request 的形式请求与upstream合并</strong></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git remote add upstream https://github.com/<span class="variable">$&#123;USER&#125;</span>/<span class="variable">$&#123;PROJECT&#125;</span>.git <span class="comment"># 运行完这一步remote的 push和 pull url 均被设置为https://github.com/$&#123;USER&#125;/$&#123;PROJECT&#125;.git</span></span><br><span class="line">git remote set-url --push upstream no_push</span><br></pre></td></tr></table></figure><ol start="4" type="1"><li>不太建议直接在main分支上进行开发，可以先创建一个更可读的分支来完成开发，一般命名为<em>feat-xxx</em></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b feat-xxx</span><br></pre></td></tr></table></figure><ol start="5" type="1"><li>在本地进行开发，更新本地分支的代码，注意每次开工之前都需要先将云端最新的更新同步下来</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git fetch upstream</span><br><span class="line">git checkout main</span><br><span class="line">git rebase upstream/main</span><br></pre></td></tr></table></figure><ol start="6" type="1"><li>正常流程add，commit，注意commit的时候<strong>一般要加上<code>-s</code>参数</strong>，用来标记证书审查。另外有的项目对commit的message格式有要求</li><li>将代码push到之前fork的云端仓库中</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin feat-xxx</span><br></pre></td></tr></table></figure><h2 id="在github中操作提出一个pr">4. 在github中操作，提出一个PR</h2><p>一般完成push之后，如果这个仓库是你fork来的，那么github会在repo头部提示让你开一个PullRequest</p><p><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo%9%AA%8C1%20%E6%88%90%E5%8A%9F%E8%AE%BF%E9%97%AEftp%E6%9C%8D%E5%8A%A1%E5%99%A8.pic.jpg" /><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202302121554069.png"alt="image.png" /></p><p>如果没有的话，也可以切换到feat-xxx分支，然后点击下方的“Contribute”按钮开启一个PR，或者直接点Issuses边上的PullRequest按钮进入对应页面</p><p>一般来说，提交的PR都有项目管理员预设的文档模板，需要根据文档格式编写提交PR的文档，使得管理员可以明晰你所做的工作，解决的问题，以及通过测试的情况，一般的模板需要包含以下内容：</p><ol type="1"><li><strong>Pre-Checklist</strong>：这里列了3个前置检查项，提醒 PR提交者要先阅读 Contributing文档，然后代码要有完善的注释或者文档，尽可能添加测试用例等；</li><li><strong>Description</strong>：这里填写的是 PR的描述信息，也就是介绍你的 PR 内容的，你可以在这里描述这个 PR解决了什么问题等；</li><li><strong>RelatedIssues</strong>：记得吗？我们在开始写代码之前其实是需要认领 issue的，这里要填写的也就是对应 issue 的 id，假如你领的 issue 链接是 <ahref="https://github.com/devstream-io/devstream/issues/796">https://github.com/devstream-io/devstream/issues/796</a>，并且这个issue 通过你这个 PR 的修改后就完成了，可以关闭了，这时候可以在 RelatedIssues 下面写“<strong>close #796</strong>”；</li><li><strong>NewBehavior</strong>：代码修改后绝大多数情况下是需要进行测试的，这时候我们可以在这里粘贴测试结果截图，这样reviewers 就能够知道你的代码已经通过测试，功能符合预期，这样可以减少review 工作量，快速合入。</li></ol><p>提交完成之后，可能会执行预设的CI，检查提交的代码，如果有的测试样例没有通过，则需要修复，直至通过所有的样例。</p><p>后续有新的变更（例如根据Reviewers的意见修改或者自行作出的变更），可以直接<code>git push origin feat-xxx</code>来进行提交，github会帮你把新增的commits全部追加到一个未合入的PR里面去。</p><h2 id="合并commit">5. 合并commit</h2><ul><li>git命令行方式合并</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 合并HEAD和HEAD指向的前一个，共两个commit，这里也可以给出commit list（版本号表示）</span></span><br><span class="line">git rebase -i HEAD~2</span><br></pre></td></tr></table></figure><p>这样根据流程先后配置需要保留和舍弃的commit、解决冲突以及修改commitmessage等，就可以合并若干个commit，生成简洁有效的变更信息了。注意，这样合并完之后再向云端push的时候，需要加入<code>-f</code>参数强制执行，冲掉之前的变更信息</p><h2 id="解决pr产生的冲突">6. 解决PR产生的冲突</h2><ul><li>在线解决冲突，可以直接在github里面查看并修改冲突的文件，需要删掉所有的 ''&lt;&lt;&lt;&lt;&lt;&lt;&lt;''、''&gt;&gt;&gt;&gt;&gt;&gt;&gt;'' 和 ''======='' 标记，只保留最终想要的内容，以这种方式解决的冲突<strong>会在github上产生一条变更记录</strong></li><li>本地解决冲突</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先切回到 main 分支</span></span><br><span class="line">git checkout main</span><br><span class="line"><span class="comment"># 拉取上游代码（实际场景肯定是和上游冲突，我们这里的演示环境其实是 origin）</span></span><br><span class="line">git fetch upstream</span><br><span class="line"><span class="comment"># 更新本地 main（这里也可以用 rebase，但是 reset 不管有没有冲突总是会成功）</span></span><br><span class="line">git reset --hard upstream/main</span><br></pre></td></tr></table></figure><p>然后我们要将 main 分支的代码合入自己的特性分支，同时解决冲突。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout feat-1</span><br><span class="line">git rebase main</span><br></pre></td></tr></table></figure><h2 id="修改commit信息">7. 修改commit信息</h2><p>用下面的命令可以修改commit信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit --amend</span><br></pre></td></tr></table></figure><p>如果是在commit的时候忘记了<code>-s</code>参数，则可以使用下面的命令自动加上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit --amend -s</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      
      <category domain="https://jiengup.github.io/tags/open/">open</category>
      
      <category domain="https://jiengup.github.io/tags/source/">source</category>
      
      
      <comments>https://jiengup.github.io/2023/12/11/17/37/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>三步走：如何阅读一篇文献</title>
      <link>https://jiengup.github.io/2023/12/11/17/32/</link>
      <guid>https://jiengup.github.io/2023/12/11/17/32/</guid>
      <pubDate>Mon, 11 Dec 2023 09:32:29 GMT</pubDate>
      
        
        
      <description>&lt;blockquote&gt;
&lt;p&gt;成为研究生已经第二年了，一直感觉自己阅读文献的效率比较低，在上国外课程的时候偶然看到老师引导研究生们去阅读这样一篇论文，论文提出了三步走的方法来做文献阅读，感觉非常受用
&lt;a
href=&quot;https://web.stanford.edu/class</description>
        
      
      
      
      <content:encoded><![CDATA[<blockquote><p>成为研究生已经第二年了，一直感觉自己阅读文献的效率比较低，在上国外课程的时候偶然看到老师引导研究生们去阅读这样一篇论文，论文提出了三步走的方法来做文献阅读，感觉非常受用<ahref="https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf">原文链接</a></p></blockquote><h2 id="引入">1. 引入</h2><p>研究人员需要阅读论文有几个原因：为了review会议或课堂上的论文，以保持自己所在领域的最新动态，或者进行对一个新领域的文献调研。一位典型的研究人员每年可能会花费数百小时来阅读论文。</p><p>高效地阅读论文是一项关键但很少被教授的技能。因此，初级研究生必须通过试错来自学。学生们在这个过程中浪费了大量精力，并经常感到沮丧。</p><p>多年来，我使用了一种简单的方法来高效地阅读论文。本文描述了“threepass”方法及其在进行文献调研时的应用。</p><h2 id="three-pass-方法">2. Three-Pass 方法</h2><p>阅读论文的核心方法是你需要阅读这篇文章 3遍，而不是从头读到尾。每一遍完成特定的目标：</p><ol type="1"><li>第一遍你需要获得这篇论文的大体思想</li><li>第二遍你需要抓住论文的内容，但不需要深入细节</li><li>第三遍你需要深入的理解这篇论文</li></ol><h3 id="第一遍">2.1 第一遍</h3><p>第一遍阅读是应该是一个快速浏览，以获得对论文的整体了解。还需要决定是否需要进行更多次阅读。这个过程应该花费大约<mark style="background: #FF5582A6;">五到十分钟</mark>，并包括以下步骤:</p><ol type="1"><li>认真的阅读标题、摘要以及引入</li><li>阅读章节和小节的标题，但是不要读其他的内容</li><li>阅读结论</li><li>瞟一眼引用，脑海里将你已经读过的那些剔除掉</li></ol><p>经过第一遍阅读之后，你应该能够回答以下的 5 个 C：</p><ol type="1"><li>Category：这篇文章的类型是什么？一篇实验论文吗？还是对现有系统的分析？或者是对研究原型的描述？</li><li>Context：哪些其他的论文和它相关？哪些理论基础被用来分析这个问题</li><li>Correctness：论文中的假设有效吗？</li><li>Contributions：这篇论文主要的贡献有哪些？</li><li>Clarity：这篇论文的写作好吗？</li></ol><p>使用这些信息，您可以选择不再阅读下去。这可能是因为论文对您没有兴趣，或者您对该领域了解不够以理解论文，或者作者做出了无效的假设。第一次浏览适用于不属于您研究领域但未来可能与之相关的论文。</p><p>顺便提一下，当你写论文时，可以预期大多数审稿人（和读者）只会对其进行一遍阅读。请注意选择连贯的章节和子章节标题，并编写简明扼要、全面的摘要。如果审稿人在第一遍阅读后无法理解主旨，那么该论文很可能会被拒绝；如果读者在五分钟内无法理解论文的亮点，那么该论文很可能永远不会被阅读。</p><h3 id="第二遍">2 .2 第二遍</h3><p>在第二遍阅读中，更加仔细地阅读论文，但忽略证明等细节。在阅读过程中，可以将关键点记下来，在边缘做出评论。</p><ol type="1"><li>请仔细查看论文中的图表、示意图和其他插图。特别注意图表是否正确标注了坐标轴？结果是否显示了误差线，以确保结论具有统计学意义？这些常见错误将区分匆忙、粗糙的工作与真正优秀的工作。</li><li>请记住将相关未读的参考文献标记为进一步阅读（这是了解论文背景的好方法）。</li></ol><p>第二遍阅读应该需要<mark style="background: #FF5582A6;">最多一个小时</mark>。在这次阅读之后，你应该能够理解论文的内容。你应该能够向他人总结论文的主要观点，并提供支持性证据。这个详细程度适用于你感兴趣但不属于你研究专长领域的论文。</p><p>有时候，即使经过第二遍阅读，你仍然无法理解一篇论文。这可能是因为论文研究的内容对你来说是全新的，术语和缩写不熟悉。或者作者使用了你不理解的证明或实验技术，导致整篇论文难以理解。论文可能写得很差，包含没有依据的断言和大量前向引用。或者可能只是因为已经很晚了而且你感到疲劳。现在你可以选择：</p><ul><li>将论文放在一边，希望不用理解这篇论文也不会影响到你的工作</li><li>稍后再回到这篇论文上，在阅读背景资料之后再次尝试</li><li>坚持下去并进行第三遍阅读。</li></ul><h3 id="第三遍">2.3 第三遍</h3><p>为了全面的理解论文，尤其是如果你要对这篇论文做 review的话，需要第三遍阅读。第三遍阅读的关键是<strong>尝试去模拟地复现这篇论文</strong>：也就是说，作和作者相同的假设，重新来一遍同样的工作。通过将这个再创作与实际论文进行比较，你不仅可以轻松地识别出一篇论文的创新之处，还可以发现它隐藏的缺陷和假设。</p><p>这一遍需要非常细致的注意。你应该在每个陈述中识别和质疑每一个假设。此外，你还应该思考一下自己如何呈现特定的想法。实际与虚拟的比较能够深入了解论文中的证明和演示技巧，并且很可能将其添加到你的工具库中。在这个过程中，你还应该记下未来工作的想法。</p><p>这个阅读过程对于初学者来说可能需要大约<mark style="background: #FF5582A6;">四到五个小时</mark>，对于有经验的读者来说大约需要一个小时。在这个阅读过程结束时，你应该能够从记忆中重建整篇论文的结构，并且能够识别出它的优点和缺点。特别是，你应该能够找出隐含的假设、相关工作中缺失的引用以及实验或分析技术可能存在的问题。</p><h2 id="做一个文献调研">3. 做一个文献调研</h2><p>在进行文献调研时，需要运用阅读论文的技巧。这将要求你阅读数十篇论文，可能是在一个陌生领域。那么应该读哪些论文呢？以下是如何使用三遍法来帮助你。</p><p>首先，使用学术搜索引擎（如 Google Scholar 或CiteSeer）和一些精选关键词找到该领域内最近的三到五篇论文。对每篇论文进行第一遍浏览以了解其工作内容，然后阅读它们的相关工作部分。你会找到最近工作的简要摘要，并且也许幸运的话还能找到最近综述性文章的指针。如果能找到这样一份综述文章，那就大功告成了！你应该庆幸自己运气不错。</p><p>否则，在第二步中，在参考文献中寻找共同引用和重复作者姓名。这些是该领域内关键论文和研究人员。下载这些关键论文并将它们放在一边。然后去关键研究人员的网站上看看他们最近在哪里发表了论文。这将帮助您确定该领域的顶级会议，因为最优秀的研究人员通常在顶级会议上发表论文。</p><p>第三步是访问这些顶级会议的网站，并查阅他们最近的论文集。快速浏览通常可以找到最近高质量的相关工作。这些论文，连同之前你留下来的那些，构成了你调研报告的第一个版本。对这些论文进行两次遍历。如果它们都引用了一篇关键性的论文而你之前没有找到，就获取并阅读它，如有必要则迭代处理。</p>]]></content:encoded>
      
      
      
      <category domain="https://jiengup.github.io/tags/Research/">Research</category>
      
      <category domain="https://jiengup.github.io/tags/Paper/">Paper</category>
      
      <category domain="https://jiengup.github.io/tags/Reading/">Reading</category>
      
      
      <comments>https://jiengup.github.io/2023/12/11/17/32/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【文献精读】FIRM: An Intelligent Fine-Grained Resource Management Framework for SLO-Oriented Microservices 微服务下基于关键节点提取和强化学习的的细粒度资源管理框架</title>
      <link>https://jiengup.github.io/2023/06/08/19/43/</link>
      <guid>https://jiengup.github.io/2023/06/08/19/43/</guid>
      <pubDate>Thu, 08 Jun 2023 11:43:38 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;metadata&quot;&gt;Metadata&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoran Qiu, Subho S Banerjee, Saurabh Jha,
Zbigniew T Kalbar</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="metadata">Metadata</h2><blockquote><ul><li><strong>Authors:</strong> Haoran Qiu, Subho S Banerjee, Saurabh Jha,Zbigniew T Kalbarczyk, Ravishankar K Iyer</li><li><strong>Cite Key:</strong> [<span class="citation"data-cites="qiuFIRMIntelligentFineGrained">[@qiuFIRMIntelligentFineGrained]</span>]</li><li><strong>Link:</strong> <ahref="file:///Users/xingguangjie/Library/Mobile%20Documents/com~apple~CloudDocs/Zotero/FIRM_.pdf">FIRM_.pdf</a></li><li><strong>Bibliography:</strong> Qiu, H., Banerjee, S. S., Jha, S.,Kalbarczyk, Z. T., &amp; Iyer, R. K. (n.d.). <em>FIRM: An IntelligentFine-Grained Resource Management Framework for SLO-OrientedMicroservices</em>.</li></ul></blockquote><h2 id="abstract">Abstract</h2><blockquote><p>User-facing latency-sensitive web services include numerousdistributed, intercommunicating microservices that promise to simplifysoftware development and operation. However, multiplexing of computeresources across microservices is still challenging in productionbecause contention for shared resources can cause latency spikes thatviolate the servicelevel objectives (SLOs) of user requests. This paperpresents FIRM, an intelligent ﬁne-grained resource management frameworkfor predictable sharing of resources across microservices to drive upoverall utilization. FIRM leverages online telemetry data andmachine-learning methods to adaptively (a) detect/localize microservicesthat cause SLO violations, (b) identify low-level resources incontention, and (c) take actions to mitigate SLO violations via dynamicreprovisioning. Experiments across four microservice benchmarksdemonstrate that FIRM reduces SLO violations by up to 16× while reducingthe overall requested CPU limit by up to 62%. Moreover, FIRM improvesperformance predictability by reducing tail latencies by up to 11×.##Annotations</p></blockquote><h2 id="main-content">Main Content</h2><p>本文提出了FIRM，一个智能的、细粒度的资源管理框架，用以预测不同微服务之间的共享资源，以提升整体的利用率。FIRM使用在线遥测技术和机器学习方法来适应性的：</p><ul><li>检测和定位触犯SLO的微服务</li><li>识别竞争中的低级资源</li><li>通过动态分配以缓解SLO触犯 其主要贡献有：</li><li>提出了一种基于SVM的SLO触犯定位方法</li><li>给予强化学习的SLO触犯缓解措施，并且能够通过使用 tranfer learning为单个微服务实例调优定制的 RL 代理</li><li>在线的训练以及性能异常注入，人工创造资源竞争的情况，由此来构造训练集</li><li>实现和验证，开源了针对k8s的FIRM框架，并且在四个真实微服务benchmark上进行了测试</li></ul><h3 id="firm-框架">FIRM 框架</h3><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305191640816.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><ol type="1"><li><em>TracingCoordinator</em>：负责收集负载和遥测数据(资源数据，1)并将它们存储在一个中心化的图数据库中，以监测关键路径的变化并从中抽取出关键微服务实例</li><li><em>Extractor</em>：监测SLO触犯，向TracingCoordinator查询抽取出的CP(2)并定位关键的SLO触犯根因微服务实例(3)</li><li><em>RL-based ResourceEstimator</em>：通过监测数据(1)以及关键实例(3)，FIRM通过强化学习模型来向关键实例重新分配资源，RL模型会综合考虑资源利用率的上下文信息（例如底层资源CPU、LLC、内存以及网络），性能信息（例如每个微服务的以及端到端的延时分布）以及负载特征（例如请求速率以及成分）等来进行决策</li><li><em>Deployment</em>：动作被验证和执行，以在底层为k8s的集群上部署</li><li><em>Performance AnomalyInjection</em>：为了更好的训练机器学习模型和强化学习智能体（扩展exploration-exploitationspace），FIRM通过主动引发资源竞争来生成SLO触犯 ### Tracing CoordinatorFIRM实现一个分布式Tracing，用于构建<em>excution historygraph</em>。<em>span</em>是指一个微服务实例处理一次请求所需要的时间，它是基于请求达到微服务到其发送恢复给调用者的时间计算的</li></ol><p>FIRM的实现受到了Jaeger的启发，每一个微服务实例都伴有一个OpenTracing组件用以追踪和计算span，因此任何伴有OpenTracing组件的微服务都能自然的集成进FIRM的tracing框架。</p><p>TracingCoordinator作为一个无服务的、多副本的数据处理组件运行在系统中，其造成的负荷可以忽略不计（小于0.4%的带宽损失和小于0.15%的延时损失）</p><h3 id="critical-path-extractorcritical-component-extractor">CriticalPath Extractor&amp;Critical Component Extractor</h3><p>使用算法1可以在execution historygraph中抽取出关键路径，算法1的核心是一个权重最长路径算法，它需要同时考虑微服务架构中的通讯和计算特征（Parallel、Sequential、Background）<imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305191725444.png"alt="image.png" />在此之后，FIRM在每个抽取出来的CP中使用一个适应性强的、数据驱动的方法来决定关键成分（微服务实例），整个过程如算法2所示。算法首先计算每一个CP和每一个实例的“特征”，表示性能的变化和拥塞程度，记者这两种特征被喂入一个SVM分类器得到二分类结果（即实例是否应该改变其资源进行重新分配）</p><p>为了达到这个目的，需要从两个方面考虑：端到端延时的变化以及单个微服务由于服务队列导致的竞争而发生的变化：</p><ul><li>Per-CPVariability：皮尔森相关系数衡量单个微服务延时对于整个端到端延时的影响力</li><li>Per-InstanceVariablity：单个微服务的拥塞强度（定义为p-99延时和延时中位数之比） <imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305201449267.png" />### Resource Estimator传统的基于性能建模和启发式的方法都存在两个问题：</li></ul><ol type="1"><li>它们没有考虑系统状态的动态性</li><li>设计、实现和验证方法需要专家知识，并且需要同时掌握微服务负载特征和底层架构强化学习在探索动作空间和生成最优策略时提供了紧密的反馈，且不需要依赖一个不准确的假设（启发式或者规则）</li></ol><p>FIRM采用[[DDPG算法]]来进行强化学习critic网络和actor网络结构如下图所示，actor网络有8个输入和5个输出，critic网络有23个输入和1个输出，它们的输入输出含义由表3所示<imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305201640055.png" /></p><p>在每一个迭代步中，每一种资源的利用率<spanclass="math inline">\(RU_t\)</span>是通过TracingCoordinator获得的遥测数据（table2）得到的，除此之外，Extractor还有收集当前的延时、请求到达速率以及请求构成（当前请求的种类），强化学习智能体会根据这些数据计算状态（table3） <imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305201611869.png"alt="image.png" /> <imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305201611604.png"alt="image.png" /></p><ul><li><spanclass="math inline">\(SM_t\)</span>：若微服务实例被定为罪魁祸首则定义为<spanclass="math inline">\(SLO\_Latency/current\_latency\)</span>，如果没有消息到达，则设定为没有SLO触犯（<spanclass="math inline">\(SM_t=1\)</span>）</li><li><spanclass="math inline">\(WC_t\)</span>：定义为当前的到达率和前一个时间戳到达率的比</li><li><spanclass="math inline">\(RC_t\)</span>定义为一个唯一的值，从一个请求占比数组中编码而来（使用numpy包）</li></ul><p>FIRM通过transferlearning来解决为每个微服务定制模型造成的庞大时间开销</p><h3 id="action-execution">Action Execution</h3><p>FIRM假设物理机资源是无穷的，因此没有设计准入和节流策略，如果一个动作会导致资源撑爆了，那么它会被替代为一个scale-out操作</p><ul><li>CPU动作：通过cgroups的<code>cpu.cfs_period_us</code>和<code>cpu.cfs_quota_us</code>实现</li><li>内存动作：通过Intel MBA和Intel CAT技术来控制内存带宽和LLC容量</li><li>I/O动作：使用cgroup中的<code>blkio</code>来控制</li><li>网络动作：使用HTB队列控制</li></ul><h3 id="performance-anomaly-injector">Performance Anomaly Injector</h3><p>通过异常注入引发SLO违背可以加速训练过程并且扩展强化学习在不利的资源竞争的空间中探索（也就是所说的exploration-exploitationtrade off）异常注入通过将二进制文件绑定到每一个容器的操作系统层实现，并且可以远程触发，而且可以随时控制注入目标，异常种类，注入时间、区间、模式和强度，注入由7种可能触发SLO违背的性能异常构成<imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305201700679.png"alt="image.png" /></p><h2 id="details">Details</h2><h3id="firm缓解slo触犯时不会过度分配-这是因为">FIRM缓解SLO触犯时不会过度分配,这是因为：</h3><ol type="1"><li>它将底层资源和应用性能之间的依赖通过基于强化学习的反馈回路进行建模，以解决不确定性和存在的噪声</li><li>它使用了一个两层的方法，在线关键路径分析和SVM模型仅仅过滤出那些需要被考虑的微服务，因此使得应用框架无关并且使得强化学习代理训练更加快速### Critical Path(CP) 微服务<span class="math inline">\(m\)</span>的一个<strong>critical path(CP)</strong>定义为由用户请求启动的，并且以微服务<spanclass="math inline">\(m\)</span>结束的最长调用路径，而如果单独说CP而不提任何微服务，那么CP就指代端到端延时</li></ol><p>通过在开源数据集上进行大量的异常注入实验，有了如下的发现： 1.CP的行为是动态的，因为底层的共享资源会发生竞争 <imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305182151634.png"alt="image.png" /> 2.有着较大负载的个体微服务通常不是导致触犯SLO的罪魁祸首，但是<strong>文章假设罪魁祸首一定在CP上(?)</strong><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305182152374.png"alt="image.png" /> 3.缓解SLO触犯的策略要根据用户负载和资源竞争情况不同而发生变化 1.一般来说，通常有两种办法可以向关键微服务提供更多资源 1. <em>scaleout</em>：在集群的另一个node上启动一个新的容器实例 2. <em>scaleup</em>：向已存在的容器显式地划分更多资源（例如内存带宽和最后一级缓存）提供更多的CPU核2. scale out和scale up之间的tradeoff不仅取决于用户负载，同样也取决于资源类别 <imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202305191641024.png" /></p>]]></content:encoded>
      
      
      
      <category domain="https://jiengup.github.io/tags/Microservice/">Microservice</category>
      
      <category domain="https://jiengup.github.io/tags/Reinforcement-Learning/">Reinforcement Learning</category>
      
      <category domain="https://jiengup.github.io/tags/Resource-Management/">Resource Management</category>
      
      <category domain="https://jiengup.github.io/tags/Method-Benchmark/">Method Benchmark</category>
      
      
      <comments>https://jiengup.github.io/2023/06/08/19/43/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【文献精读】The Power of Prediction: Microservice Auto Scaling via Workload Learning 微服务下的负载预测及资源分配</title>
      <link>https://jiengup.github.io/2023/04/14/19/10/</link>
      <guid>https://jiengup.github.io/2023/04/14/19/10/</guid>
      <pubDate>Fri, 14 Apr 2023 11:10:22 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;metadata&quot;&gt;Metadata&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shutian Luo, Huanle Xu, Kejiang Ye, Guoyao
Xu, Liping Zhang, Guodong Yang</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="metadata">Metadata</h2><ul><li><strong>Authors:</strong> Shutian Luo, Huanle Xu, Kejiang Ye, GuoyaoXu, Liping Zhang, Guodong Yang, Chengzhong Xu</li><li><strong>Series:</strong> [[SoCC '22]]</li><li><strong>Cite Key:</strong> [<span class="citation"data-cites="luoPowerPredictionMicroservice2022">[@luoPowerPredictionMicroservice2022]</span>]</li><li><strong>Link:</strong> <ahref="https://doi.org/10.1145/3542929.3563477">The_power_of_prediction_Proceedings_of_the_13th_Symposium_on_Cloud_Computing_2022.pdf</a></li><li><strong>Bibliography:</strong> Luo, S., Xu, H., Ye, K., Xu, G.,Zhang, L., Yang, G., &amp; Xu, C. (2022). The power of prediction:Microservice auto scaling via workload learning. <em>Proceedings of the13th Symposium on Cloud Computing</em>, 355–369. <ahref="https://doi.org/10.1145/3542929.3563477">https://doi.org/10.1145/3542929.3563477</a></li><li><strong>Tags:</strong> #microservices, #proactive-auto-scaler,#workload-uncertainty-learning</li></ul><h2 id="abstract">Abstract</h2><p>When deploying microservices in production clusters, it is criticalto automatically scale containers to improve cluster utilization andensure service level agreements (SLA). Although reactive scalingapproaches work well for monolithic architectures, they are notnecessarily suitable for microservice frameworks due to the long delaycaused by complex microservice call chains. In contrast, existingproactive approaches leverage end-to-end performance prediction forscaling, but cannot effectively handle microservice multiplexing anddynamic microservice dependencies. In this paper, we present Madu, aproactive microservice auto-scaler that scales containers based onpredictions for individual microservices. Madu learns workloaduncertainty to handle the highly dynamic dependency betweenmicroservices. Additionally, Madu adopts OS-level metrics to optimizeresource usage while maintaining good control over scaling overhead.Experiments on large-scale deployments of microservices in Alibabaclusters show that the overall prediction accuracy of Madu can reach ashigh as 92.3% on average, which is 13% higher than the state-of-the-artapproaches. Furthermore, experiments running real-world microservicebenchmarks in a local cluster of 20 servers show that Madu can reducethe overall resource usage by 1.7X compared to reactive solutions, whilereducing end-to-end service latency by 50%.</p><h2 id="main-content">Main Content</h2><p>本工作提出了一种基于单个微服务不确定性分析的OSmetric负载预测方案Madu，用以实现微服务场景下的资源分配，以提高资源利用率，保证时延的稳定### Madu的背景和贡献</p><h4 id="reactive-proactive">Reactive &amp; Proactive</h4><p>Reactive的资源调整方法对于微服务框架来说是不可行的，因为微服务的调用链非常复杂，会产生很长的延迟效应，而现有的Proactive的方法也同样没有考虑到微服务的<strong>复用性(microservicemultiplexing)</strong> 和<strong>动态依赖性(mircroservicedependencies)</strong>。</p><p>Q：在微服务下， 为什么Reactive的scaling方法不work？ A： 1.微服务处理服务请求的调用链可能非常长，这种情况下，调用链底端的微服务可能需要很长时间才会经历负载的变化，而当它观测到负载变化时，可能已经为时过晚了。2.scaling微服务通常需要从仓库拉去images，这个操作需要数秒来完成，而在线服务的SLAs通常是用几百微秒来定义的，因此reactive的方法常常会导致SLA侵犯。</p><p>Q：现有的Proactive的scaling方法为什么不能有效的应用于微服务场景？今的微服务预测通常是基于依赖图来进行预测，这种方法没有考虑微服务的两个核心特征：1.同一个在线服务可能会产生完全不同拓扑结构的调用图，这种高度动态的特征使得基于图的训练难以在实际中使用(Fig1(a): Dynamicdependencies)，现有的proactive的autocaling的方法都是基于静态图的所以效果不会好。2.单个的microservice可能会被多个在线服务复用，如果独立的训练所有的调用图可能难以解决微服务复用的问题(Fig1(b): Sharedmicroservices)，一种方法是组合单个服务图形成一个组合图，但是这种方法的可扩展性比较差，因为生产环境通常会包含成千上万个微服务。<imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304132221312.png"alt="image.png" /></p><h4 id="data-dependent-uncertainty">Data-dependent Uncertainty</h4><p>一个微服务负载除了有周期特征之外，还有很强的[[Data-depententUncertainty]]。本文给出的Data-dependent Uncertainty的解释是</p><blockquote><p>That is, peak workloads have much higher variance across differentperiods (relative to the mean workload) than other workloads due to thedynamic dependency between microservices.</p></blockquote><p>其实这个所谓的Data-depententUncertainty并不是微服务独有的，关于这个部分，非常值得深入研究。因为时序预测中这个现象非常普遍，需要去找一下有没有什么更先进的方法能解决这种不确定性问题。</p><h4 id="微服务负载的不确定性">微服务负载的不确定性</h4><p>本工作将<strong>负载</strong>定义为<strong>每分钟调用次数(Call Perminute, CPM)</strong></p><p><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304132235191.png" /></p><ul><li>微服务的负载有着周期特征，例如说，在工作日的时候峰值总是在同一时间出现(Fig2 (a))</li><li>然而，微服务的负载也有着非常强的[[Data-depententUncertainty]]，也就是说，负载在不同周期内的方差与平均负载有关。(Fig2(b))所示，对于通信微服务，其在不同工作日的峰值工作负载比其他服务的负载具有更高的方差。本文通过计算在30天内不同周期内工作负载（相对于平均工作负载）的方差来量化阿里巴巴集群中每个微服务的工作负载不确定性</li><li>由[[luoCharacterizingMicroserviceDependency2021|阿里巴巴微服务trace分析]]可知，微服务的动态依赖性与微服务的不确定性有很强的相关性，不同的微服务的动态依赖不同，因此不确定性不是统一的</li></ul><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304141056120.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><h4 id="微服务性能指标">微服务性能指标</h4><p><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304141100677.png"alt="image.png" />CPU和内存利用率与微服务工作负载的相关性比微服务响应时间更强。因此，Madu转而使用这些操作系统级别的指标来评估微服务的性能以进行资源扩展</p><h4 id="本文的贡献">本文的贡献</h4><ul><li>设计了一个新的模型来学习微服务负载的不确定性，它可以捕捉微服务间动态的负载以及克服低谷高峰负载的情况</li><li>实现了一种新的proactive的微服务资源自动缩放器，可以以很低的开销实现高效的缩放微服务容器</li><li>执行了真实负载重放（Alibaba）来验证负载预测的能力论文提出的Madu是一个以容器为粒度的资源自动缩放器，它能够同时学习微服务负载的均值和不确定性，并且精妙的结合这两个部分以产生更加精确的预测结果。</li></ul><h3 id="系统总体设计">系统总体设计</h3><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304141116309.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><p>Auto-scaler独立于微服务集群运行，在<strong>每一分钟</strong>的开始，它会向</p><ul><li><strong>WorkloadPredictor</strong>查询未来会被发送至每个微服务的调用次数（预测的负载）</li><li>Auto-scaler会使用<strong>UtilizationAnalysis</strong>来决定每一个时间区间内的需要部署的初始容器数量以控制CPU和内存利用率</li><li>每个微服务的初始资源利用情况估计是由<strong>Performance ProfilingModel</strong>完成的，但是由于微服务负载震荡得很厉害，所以并不会实际使用这个初始的决策</li><li>初始决策被发送至<strong>AutoscalingOptimization</strong>来决定最后需要缩放的容器数量，它会保证不会频繁的调动资源</li></ul><h3 id="madu的设计与实现">Madu的设计与实现</h3><h4 id="微服务负载学习">微服务负载学习</h4><ul><li><p>Madu不是基于微服务间的依赖来学习负载的，因为生产环境下有成百上千的微服务之间存在依赖，这样会引入相当大的开销</p></li><li><p>Madu是基于[[seq2seq]]的，Madu针对其主要有两个改进：</p><ul><li>提出了一种新的损失函数</li><li>引入了随机注意力机制(stochastic [<a href="#attention">attention</a>]mechanism)</li></ul></li></ul><p><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304141739890.png" /></p><p>模型输入(Encoder)：长度为n的向量<spanclass="math inline">\(\boldsymbol{X}_t^E=\left[x_{t-n+1}, x_{t-n+2},\cdots, x_t\right]\)</span> 模型输出(Decoder)：长度为m的向量<spanclass="math inline">\(\boldsymbol{X}_t^D=\left[x_{t+1}, x_{t+2}, \cdots,x_{t+m}\right]\)</span> 其中的每一个<spanclass="math inline">\(x_i={C_t,t_m,t_h,t_w}\)</span>，<spanclass="math inline">\(C_t\)</span>表示<spanclass="math inline">\(t\)</span>时刻的CPM，而<spanclass="math inline">\(t_m,t_h,t_w\)</span>分别表示分钟、小时和星期，送入时间特征让模型能够学习到时序信息，n和m的值是训练准确率和scaling效率的tradeoff</p><ul><li>似然函数：由于data-depentent uncertainty的存在，需要假设<spanclass="math inline">\(\boldsymbol{X}_t^D=f(\boldsymbol{X}_t^E)+\epsilon(\boldsymbol{X}_t^E)\)</span>，其中<spanclass="math inline">\(\epsilon\)</span>属于(01)正态分布，因此似然函数可以定义为<spanclass="math inline">\(p\left(\boldsymbol{X}_t^D \mid \boldsymbol{X}_t^E,\boldsymbol{W}\right)=\exp\left(-\frac{\left(\boldsymbol{X}_t^D-f\left(\boldsymbol{X}_t^E\right)\right)^2}{2\sigma^2\left(\boldsymbol{X}_t^E\right)}\right) / \sqrt{2 \pi\sigma^2\left(\boldsymbol{X}_t^E\right)}\)</span>，其中<spanclass="math inline">\(f(.)\)</span>和<spanclass="math inline">\(\epsilon(.)\)</span>为seq2seq模型拟合的函数，学习过程中模型需要调整参数<spanclass="math inline">\(\boldsymbol{W}\)</span>来极大似然函数</li><li>随机注意力机制：注意力分数<spanclass="math inline">\(S\)</span>是从一个高斯分布中采样来的，这个高斯分布是基于<spanclass="math inline">\(\boldsymbol{X}_t^E\)</span> (for data-dependentuncertainty)和参数<spanclass="math inline">\(\boldsymbol{W}\)</span>的，引入了注意力机制之后，预测结果就可以表示为<spanclass="math inline">\(\left[\widehat{\mu}_{t+1},\widehat{\sigma}_{t+1}\right]=\left[f\left(\widehat{\mu}_t,\widehat{\sigma}_t, h_t, c_t, c v_t\right), \sigma\left(\widehat{\mu}_t,\widehat{\sigma}_t, h_t, c_t, cv_t\right)\right]\)</span>了，两个函数的前4项是普通LSTM的组件，而<spanclass="math inline">\(cv_t\)</span>由注意力机制产生 <spanclass="math display">\[\begin{gather}cv_t=\sum_{j=1}^{n}a_{tj}h^E_t \\\alpha_{t j}=\frac{\exp \left(s_{t j}\right)}{\sum_{k=1}^n \exp\left(s_{t k}\right)} \\s_{t j}=f\left(a_{t j}\right), a_{t j}=\tanh \left(W_a h_{t-1}+U_ah_j\right)\end{gather}\]</span></li></ul><p>对于<span class="math inline">\(s_{t j}=f\left(a_{tj}\right)\)</span>，非随机的注意力机制通常假定<spanclass="math inline">\(f\)</span>是一个线性函数，而对于随机注意力机制，则是从一个高斯分布<spanclass="math inline">\(s_{t j} \sim \mathbf{N}\left(\mu_{t j}, \sigma_{tj}\right)\)</span>中采样得到的，其中<spanclass="math inline">\(\mu_{tj}=W_{\mu j}a_{tj}+b_{\mu j},\sigma_{tj}=W_{\mu j}a_{tj}+b_{\sigma j}\)</span>，4个<spanclass="math inline">\(W\)</span>和<spanclass="math inline">\(b\)</span>是由模型学习的参数</p><h4 id="微服务性能建模">微服务性能建模</h4><p>通过刻画内存利用率以及CPU利用率与CPM的关系可以发现，它们几乎是线性相关的，因此拟合两个线性回归模型就可以了<imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304141750094.png"alt="image.png" /></p><h4 id="在线容器缩放以及缩放优化">在线容器缩放以及缩放优化</h4><p>根据Performance Profiling拟合出来的线性函数、及WorkloadPredictor预测的CPM值以及预定的CPU和内存利用率阈值，可以计算出之后m个时间区间内所需要的容器数量，但是同时<strong>为了避免大量的调整容器数量（调整容器也同样需要消耗资源并且需要一定的时间）</strong>，还需要做出一些限制，具体的，需要解决一个最优化问题：<span class="math inline">\(\begin{aligned} &amp; \min_{\boldsymbol{x}_i} \sum_{k=1}^m\left(x_i(t+k-1)-x_i(t+k)\right)^2 \\&amp; \text { s.t., } c_i(t+k) \leq x_i(t+k) \leq(1+\rho) \cdot c_i(t+k).\end{aligned}\)</span> 其中<spanclass="math inline">\(c_i\)</span>表示微服务i的容器数量，<spanclass="math inline">\(\rho\)</span>是一个预先定义的常量，这是一个[[凸优化问题]]，Madu使用[[CVX sovler]]来解</p><h3 id="验证与性能表现">验证与性能表现</h3><p>Madu和其他4种算法进行了对比，分别为ARIMA、Seq2Seq、BNN、DUBNN，<strong>值得注意的是，在高负载的情况下，DUBNN的表现也很好，但是DUBNN的训练成本要远高于Madu，并且文章解释DUBNN和BNN都需要一个先验估计，这个先验估计选取得不恰当的话会严重影响性能</strong>整体表现上：</p><ul><li>低负载情况下，Madu可以将预测准确性提高到90%以上。</li><li>高负载情况下，Madu比DUBNN要高出1个百分点，这说明Madu有处理突发负载的能力单个表现上（对于预测非常精确的样本（1±10%)*groundtruth)：-Madu在低负载情况的表现也没有很好，但是超过了其他算法很多，这是由于<strong>Madu仍然会因为不确定性学习做出过度估计</strong><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304141804969.png"alt="image.png" /></li></ul><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304141810548.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304141814703.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><h2 id="appendix">Appendix</h2><h3 id="reading-list">Reading List</h3><h4 id="reactive-autoscaling">Reactive Autoscaling</h4><ul><li>Alim Ul Gias, Giuliano Casale, and Murray Woodside. 2019.ATOM:Model-Driven Autoscaling for Microservices. In Proceedings ofICDCS.</li><li>Ram Srivatsa Kannan, Lavanya Subramanian, Ashwin Raju, JeongseobAhn, and Jason Mars. 2019. GrandSLAm: Guaranteeing SLAs for Jobs inMicroservices Execution Frameworks. In Proceedings of EuroSys.</li><li>Amirhossein Mirhosseini, Sameh Elnikety, and Thomas F Wenisch. 2021.Parslo: A Gradient Descent-based Approach for Near-optimal Partial SLOAllotment in Microservices. In Proceedings of SoCC.</li><li>Guangba Yu, Pengfei Chen, and Zibin Zheng. 2019. Microscaler:Automatic Scaling for Microservices with an Online Learning Approach. InProceedings of ICWS.</li><li>Laiping Zhao, Yanan Yang, Kaixuan Zhang, Xiaobo Zhou, Tie Qiu, KeqiuLi, and Yungang Bao. 2020. Rhythm: component-distinguishable workloaddeployment in datacenters. In Proceedings of EuroSys.</li></ul><h4 id="proactive-autoscaling">Proactive Autoscaling</h4><ul><li>Vivek M Bhasi, Jashwant Raj Gunasekaran, Prashanth Thinakaran, CyanSubhra Mishra, Mahmut Taylan Kandemir, and Chita Das. 2021. Kraken:Adaptive Container Provisioning for Deploying Dynamic DAGs in ServerlessPlatforms. In Proceedings of SoCC.</li><li>Ka-Ho Chow, Umesh Deshpande, Sangeetha Seshadri, and Ling Liu. 2022.DeepRest: Deep Resource Estimation for Interactive Microservices. InProceedings of EuroSys.</li><li>Yu Gan, Yanqi Zhang, Kelvin Hu, Dailun Cheng, Yuan He, MeghnaPancholi, and Christina Delimitrou. 2019. Seer: Leveraging Big Data toNavigate the Complexity of Performance Debugging in Cloud Microservices.In Proceedings of ASPLOS.</li><li>Haoran Qiu, Subho S. Banerjee, Saurabh Jha, Zbigniew T. Kalbarczyk,and Ravishankar K. Iyer. 2020. FIRM: An Intelligent Fine-grainedResource Management Framework for SLO-Oriented Microservices. InProceedings of OSDI.</li></ul><h4 id="workload-learning">Workload Learning</h4><ul><li>Marcus Carvalho, Walfredo Cirne, et al. 2014. Long-term SLOs forreclaimed cloud computing resources. In Proceedings of SoCC.</li><li>Lin Ma, Dana Van Aken, et al. 2018. Query-based workload forecastingfor self-driving database management systems. In Proceedings ofSIGMOD.</li><li>Ashraf Mahgoub, Alexander Medoff, Rakesh Kumar, Subrata Mitra, AnaKlimovic, Somali Chaterji, and Saurabh Bagchi. 2020. OPTIMUSCLOUD:Heterogeneous Configuration Optimization for Distributed Databases inthe Cloud. In Proceedings of ATC.</li><li>Hiep Nguyen, Zhiming Shen, et al. 2013. AGILE: Elastic DistributedResource Scaling for Infrastructure-as-a-Service. In Proceedings ofICAC.</li><li>Zhiming Shen, Sethuraman Subbiah, et al. 2011. Cloudscale: elasticresource scaling for multi-tenant cloud systems. In Proceedings ofSoCC.</li></ul><h4 id="microservice-study">Microservice Study</h4><ul><li>Shutian Luo, Huanle Xu, Chengzhi Lu, Kejiang Ye, Guoyao Xu, LipingZhang, Jian He, and Cheng-Zhong Xu. 2022. An In-depth Study ofMicroservice Call Graph and Runtime Performance. IEEE Transactions onParallel and Distributed Systems (2022).</li></ul><h4 id="attention">attention</h4><ul><li>Jay Heo, Hae Beom Lee, et al. 2018. Uncertainty-aware attention forreliable interpretation and prediction. In Proceedings of NeurIPS.</li><li>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attendand tell: Neural image caption generation with visual attention. InProceedings of ICML.</li></ul><h4 id="autoscaling">Autoscaling</h4><ul><li><ol start="2022" type="1"><li>Kubernetes Horizon Pod Autoscaler.https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/.</li></ol></li><li>Ataollah Fatahi Baarzi and George Kesidis. 2021. Showar:Right-sizing and efficient scheduling of microservices. In Proceedingsof the ACM Symposium on Cloud Computing.</li><li>Md Rajib Hossen, Mohammad A Islam, and Kishwar Ahmed. 2022.Practical Efficient Microservice Autoscaling with QoS Assurance. InProceedings of HPDC</li></ul>]]></content:encoded>
      
      
      <category domain="https://jiengup.github.io/categories/Research/">Research</category>
      
      
      <category domain="https://jiengup.github.io/tags/Microservice/">Microservice</category>
      
      <category domain="https://jiengup.github.io/tags/Machine-Learning/">Machine Learning</category>
      
      <category domain="https://jiengup.github.io/tags/Workload-Prediction/">Workload Prediction</category>
      
      
      <comments>https://jiengup.github.io/2023/04/14/19/10/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【文献精读】Characterizing-Microservice-Dependency-and-Performance-阿里巴巴微服务trace分析</title>
      <link>https://jiengup.github.io/2023/04/05/15/45/</link>
      <guid>https://jiengup.github.io/2023/04/05/15/45/</guid>
      <pubDate>Wed, 05 Apr 2023 07:45:22 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;metadata&quot;&gt;Metadata&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shutian Luo, Huanle Xu, Chengzhi Lu,
Kejiang Ye, Guoyao Xu, Liping Zhang,</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="metadata">Metadata</h2><ul><li><strong>Authors:</strong> Shutian Luo, Huanle Xu, Chengzhi Lu,Kejiang Ye, Guoyao Xu, Liping Zhang, Yu Ding, Jian He, ChengzhongXu</li><li><strong>Series:</strong> SoCC '21</li><li><strong>Cite Key:</strong> <span class="citation"data-cites="luoCharacterizingMicroserviceDependency2021">@luoCharacterizingMicroserviceDependency2021</span></li><li><strong>Link:</strong> <ahref="https://dl.acm.org/doi/10.1145/3472883.3487003">Characterizing_Microservice_Dependency_and_Performance_Proceedings_of_the_ACM_Symposium_on_Cloud_Computing_2021.pdf</a></li><li><strong>Bibliography:</strong> Luo, S., Xu, H., Lu, C., Ye, K., Xu,G., Zhang, L., Ding, Y., He, J., &amp; Xu, C. (2021). CharacterizingMicroservice Dependency and Performance: Alibaba Trace Analysis.<em>Proceedings of the ACM Symposium on Cloud Computing</em>, 412–426.<ahref="https://doi.org/10.1145/3472883.3487003">https://doi.org/10.1145/3472883.3487003</a></li></ul><h2 id="abstract">Abstract</h2><blockquote><p>Loosely-coupled and light-weight microservices running in containersare replacing monolithic applications gradually. Understanding thecharacteristics of microservices is critical to make good use ofmicroservice architectures. However, there is no comprehensive studyabout microservice and its related systems in production environments sofar. In this paper, we present a solid analysis of large-scaledeployments of microservices at Alibaba clusters. Our study focuses onthe characterization of microservice dependency as well as its runtimeperformance. We conduct an in-depth anatomy of microservice call graphsto quantify the difference between them and traditional DAGs ofdata-parallel jobs. In particular, we observe that microservice callgraphs are heavy-tail distributed and their topology is similar to atree and moreover, many microservices are hot-spots. We reveal threetypes of meaningful call dependency that can be utilized to optimizemicroservice designs. Our investigation on microservice runtimeperformance indicates most microservices are much more sensitive to CPUinterference than memory interference. To synthesize more representativemicroservice traces, we build a mathematical model to simulate callgraphs. Experimental results demonstrate our model can well preservethose graph properties observed from Alibaba traces.## Annotations</p></blockquote><p>%% begin annotations %%</p><h2 id="main-content">Main Content</h2><h3 id="微服务架构">2. 微服务架构</h3><h4 id="调用图">2.1 调用图</h4><p><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304041941287.png" /></p><p>微服务可以被分成两类：</p><ul><li>Stateful：例如Memcached和databases，通常提供读写数据的接口</li><li>Stateless：提供不同目的的各种接口微服务之间会有三种类型的通信方式：</li><li>远程调用（RPCs）：无向边，发送RPC请求的服务通常需要等待回应（同步方式）</li><li>消息队列（MQ）：有向边，发送方会将消息发送给一个第三方做持久化，接收方则会从第三方获取消息，并且不会返回Response（异步方式）</li><li>进程间通信（IP）：经常发生在同一个节点的stateless和stateful的服务之间</li></ul><p><em>通过MQ通常可以将微服务图划分为若干个部分，每个部分又可以划分为若干个<strong>两层调用（two-tierinvocations）</strong>，调用方称为（Upstream Microservice,UM），被调用方（Downstream Microservice，DM）</em></p><h4 id="图学习算法图聚类">2.2 图学习算法（图聚类）</h4><p>[[InfoGraph]]是一种非监督学习的方法，将节点信息（微服务类型）以及边特征（通信框架）作为深度神经网络的输入训练，以最大化训练集中的互信息，以此可以为每一个图生成一个embedded向量。再利用生成的向量利用K-means算法完成聚类，使用Silhouettescore度量k值的优劣。</p><p><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304051435418.png" /></p><h3 id="调用图剖析">3. 调用图剖析</h3><h4 id="微服务调用图的大小遵循重尾分布如burr-distribution">3.1微服务调用图的大小遵循重尾分布（如Burr Distribution）</h4><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304042029026.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><ul><li>大多数调用图仅仅包含少量的微服务，但同时，也有不可忽视的一部分调用图很大很深</li><li>有超过10%的调用图包含超过40种不同的微服务，在这些largesize的调用图中，有50%的微服务是Memcacheds</li><li>常见的调用深度是3，因为往往调用过程中会直接去Memcacheds中取数据，所以通常的调用图都很短<strong>（比传统的batch applications要短）</strong></li><li>但同时，也有4%的调用图的调用深度会超过10，但是当微服务数量再上升的时候，微服务的深度却不会再加深了（c图）</li></ul><h4id="微服务调用图的结构更类似一个树并且其中很多都有一个较长的调用链">3.1微服务调用图的结构更类似一个树，并且其中很多都有一个较长的调用链</h4><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304042034612.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><ul><li>大多数微服务仅仅只有一个出度（只有一个UM）</li><li>有99%的微服务的出度不超过3，入度符合一个重尾分布</li><li>大量的层（tire）只有一个服务，因此，很多比较深的调用图可以被表示为一个长链，<strong>对于这些图来说，找到微服务中的瓶颈是相对来说比较容易的</strong></li><li>有很多的stateless的服务是热点，聚合调用（agregatecalls）统计显示有超过5%的服务出现在了90%的调用图中并且处理了95%的任务</li></ul><h4 id="微服务调用图是高度动态的">3.2 微服务调用图是高度动态的</h4><p>每当一个用户请求到达EnteringMicroservice，接下来的调用路径会变得非常复杂，因为这还要取决于用户的状态（例如在为用户排序商品的时候，用户是否有优惠卷会影响排序的调用路径），这更加强调了微服务中需要基于图的预测任务。</p><h4 id="剖析">3.3 剖析</h4><p>Stateless 微服务的调用特征可以分为3种类型（Fig 8-a）：</p><ul><li>blackhole：调用图终止于此，不会再产生新的分叉</li><li>relays：一定会调用其他服务用于响应用户请求</li><li>normal：以一定概率调用DMs的服务微服务中每一层这三种类型节点的分布都不一样，每一层normal服务的调用概率分布也不一样</li></ul><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304042331490.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><p>随着调用图越来越深，Stateless访问Memcacheds的IP会线性的下降，因为调用图深的请求往往是发生了Memcacheds 的cachemiss，因此会去Database里面获取数据，IP(S2D)呈上升趋势，而S2D的上升和S2M下降的差距会由MQ来填补上。</p><h3 id="stateless微服务间的依赖分析">4. Stateless微服务间的依赖分析</h3><h4 id="循环依赖">4. 1 循环依赖</h4><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304051321296.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><ul><li>微服务的所有依赖中，循环依赖占到了超过7.8%</li><li>循环依赖可以分为两种类型：<ul><li>strong dependency：到达UM的调用和DM回复调用接口是相同的（<spanclass="math inline">\(I_1=I_3\)</span> in Fig 9），并且存在strongdependency循环调用的两个微服务间发生循环调用的概率高达83%。这种情况下，如果设计不当很可能会造成微服务间的死锁</li><li>weak depency：<span class="math inline">\(I_1 \ne I_3\)</span> inFig 9</li></ul></li><li>很少存在多个微服务参与的循环调用（long cycliccalls），因此通常只用考虑两层的cyclicdenpendency及其优化（可以考虑将存在cyclicdependency的两个微服务合并）</li></ul><h4 id="耦合依赖具有很高调用概率和很多调用次数的依赖">4.2耦合依赖——具有很高调用概率和很多调用次数的依赖</h4><p>为一个两层调用<spanclass="math inline">\(Y-&gt;X\)</span>定义两个metric：</p><p><span class="math display">\[\begin{aligned}Call Probability(Y2X) = Count(X) / Sum \\Call Time(Y2X) = Count(X) / N\end{aligned}\]</span></p><p>其中<span class="math inline">\(Count(X)\)</span>表示UM Y调用DMX的次数（注意在一次请求中一个DM可能会被Y调用多次），<spanclass="math inline">\(Sum\)</span>表示Y在所有调用图中发起的两层调用的个数和，<spanclass="math inline">\(N\)</span>表示<spanclass="math inline">\(Sum\)</span></p><p>中DM X被调用的那些两层调用的个数。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304051404843.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><p>从Fig 10中可以看出来，有超过10%的微服务对的Call Time和CallProbability的乘积大于5，这说明有相当一部分微服务对有着很强的耦合依赖。</p><p>同时还发现，有强耦合依赖的微服务对中有17%不会与其他微服务分享DM（入度为1），这意味着<strong>可以将这对服务本地化，以减少远程调用产生的开销</strong></p><h3 id="微服务运行时性能">5 微服务运行时性能</h3><h4 id="微服务调用率">5.1 微服务调用率</h4><ul><li>微服务调用率（Microservice call rate,MCR，每分钟微服务收到的调用数）和CPU利用率以及YoungGC高度相关，但是和内存利用率不高度相关，这说明<strong>CPU利用率和YoungGCs能够更好的反应运行微服务容器的资源压力</strong></li><li>相对来说，内存使用情况在大多数容器中都是非常稳定的，与先前的研究结果相符</li></ul><h4 id="微服务时延性能rt">5.2 微服务时延性能（RT）</h4><ul><li>一个在线服务的端到端时延（RT, entering调用的返回时间）在具有类似拓扑结构的调用图中很相似，但是具有不同拓扑结构的调用图之间有很大差异，<strong>这也说明了图学习算法可以用来预测延时</strong><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304051446758.png"alt="image.png" /></li><li>时延性能可能会随着CPU利用率的增加而严重退化 <imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202304051446662.png"alt="image.png" /></li><li>按照建模的观点来看，RT应该是MCR和分配资源的一个函数，所以当分配资源和调用图拓扑一定时，RT应该只与MCR有关，但是AlibabaTrace中的RT却不怎么随MCR的增加而变化，这是因为AlibabaCluster中的大多数调用都会被立即处理而不会产生排队时延，并且大多数容器中CPU的利用率都小于10%，这也进一步说明了在微服务资源利用率的优化方面还有很大的空间。</li></ul><h3 id="微服务图的概率模型生成">6. 微服务图的概率模型生成</h3><p>文章还提出了一个概率模型去生成微服务图以及调用trace，可以更好的模拟生产级别的微服务架构和用户请求。</p>]]></content:encoded>
      
      
      <category domain="https://jiengup.github.io/categories/Research/">Research</category>
      
      
      <category domain="https://jiengup.github.io/tags/Microservice/">Microservice</category>
      
      <category domain="https://jiengup.github.io/tags/Characterization/">Characterization</category>
      
      <category domain="https://jiengup.github.io/tags/Banchmark/">Banchmark</category>
      
      
      <comments>https://jiengup.github.io/2023/04/05/15/45/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>深入浅出Disk-IO</title>
      <link>https://jiengup.github.io/2023/02/25/15/38/</link>
      <guid>https://jiengup.github.io/2023/02/25/15/38/</guid>
      <pubDate>Sat, 25 Feb 2023 07:38:16 GMT</pubDate>
      
        
        
      <description>&lt;blockquote&gt;
&lt;p&gt;了解IO的工作原理、应用场景以及算法的均衡考量，深入理解存储系统可以帮助开发者和操作人员更好的工作：他们可以根据他们正在评估的数据库底层来做出更好的决定。通过将它们的工作负载与他们选择的数据库进行比较，可以解决数据库行为不当时的性能问题，并调整它们</description>
        
      
      
      
      <content:encoded><![CDATA[<blockquote><p>了解IO的工作原理、应用场景以及算法的均衡考量，深入理解存储系统可以帮助开发者和操作人员更好的工作：他们可以根据他们正在评估的数据库底层来做出更好的决定。通过将它们的工作负载与他们选择的数据库进行比较，可以解决数据库行为不当时的性能问题，并调整它们的设计结构（通过负载均衡、切换到不同的介质、文件系统、操作系统，或选择一个不同的索引类型等等）。</p></blockquote><p>如今网络的IO常常被讨论，而文件系统的IO似乎没有得到足够的关注。一部分原因是网络IO有着很多特性以及实现细节，根据操作系统的不同而发生变化。然而文件系统的IO往往是一个比较小的工具集合，同时，在现代系统中人们常用<em>数据库</em>作为存储工具，应用通过数据库提供的驱动经过网络互相交流，而文件系统IO则是留给了数据库开发人员理解以及使用。然而理解数据是怎么样被写到硬盘上以及如何从硬盘上读取仍然是一件非常重要的事情。</p><p>对于IO来说通常有几种方式（接口）</p><ul><li>系统调用：<ahref="http://man7.org/linux/man-pages/man2/open.2.html">open</a>, <ahref="http://man7.org/linux/man-pages/man2/write.2.html">write</a>, <ahref="http://man7.org/linux/man-pages/man2/read.2.html">read</a>, <ahref="http://man7.org/linux/man-pages/man2/fsync.2.html">fsync</a>, <ahref="http://man7.org/linux/man-pages/man2/sync.2.html">sync</a>, <ahref="http://man7.org/linux/man-pages/man2/close.2.html">close</a></li><li>标准IO：<a href="https://linux.die.net/man/3/fopen">fopen</a>, <ahref="https://linux.die.net/man/3/fwrite">fwrite</a>, <ahref="https://linux.die.net/man/3/fread">fread</a>, <ahref="https://linux.die.net/man/3/fflush">fflush</a>, <ahref="https://linux.die.net/man/3/fclose">fclose</a></li><li>向量化IO：<ahref="https://linux.die.net/man/2/writev">writev</a>, <ahref="https://linux.die.net/man/2/readv">readv</a></li><li>内存映射IO：- <ahref="http://man7.org/linux/man-pages/man2/open.2.html">open</a>, <ahref="http://man7.org/linux/man-pages/man2/mmap.2.html">mmap</a>, <ahref="http://man7.org/linux/man-pages/man2/msync.2.html">msync</a>, <ahref="http://man7.org/linux/man-pages/man2/munmap.2.html">munmap</a></li></ul><p>我们先从标准标准IO开始讨论，包括它的几个面向用户的优化，因为标准IO通常是应用程序开发者使用最多的。</p><h2 id="buffered-io">Buffered IO</h2><p>当谈论到<em>stdio.h</em>中函数的时候，“buffering”显得有些迷惑人。当你使用标准IO的时候，可以在全缓冲、行缓冲或者不使用任何缓冲等等方式中选择。这种“用户空间”的缓冲与内核缓冲无关（页缓存），你也可以认为缓冲（buffering）和缓存（caching）中存在不同，这可能会使得你对这些概念更加清晰。</p><h2 id="sectorblockpage">Sector/Block/Page</h2><p><em>块设备</em>是一个特殊的文件类型，它向硬件设备例如机械硬盘或者固态硬盘提供缓冲访问方式。块设备通过操作片区（Sector）来工作，Sector就是一些临近连续的字节。大多数硬盘设备将512字节作为Sector的大小。Sector是块设备传输设备的最小单元，传输一个小于Sector大小的数据是不可能的。然而，通常同时取用多个临近的段是可以的。文件系统中的最小可寻址单元是块（Block）。Block是被设备驱动请求的一些临近的Sector组。典型的块大小是512，1024，2048以及4096字节，通常IO是通过虚拟地址进行的，虚拟地址会将请求的文件系统块缓存在内存中，作为一个缓冲区提供快速的操作，虚拟内存是基于页表工作的，映射到文件系统的块中，典型的页（Page）大小是4096字节。</p><p>总的来说，虚拟内存中的Pages映射到文件系统中的Blocks，而Blocks又映射到块设备的Sectors</p><h2 id="standard-io">Standard IO</h2><p>标准IO使用<em>read()</em>和<em>write()</em>系统调用来完成IO操作，当读取数据时，PageCache首先被访问，如果数据不在cache中，则会触发一个缺页中断，然后操作系统将需要访问的内容读取到Cache中来。这意味着读操作，如果访问没有被映射的部分将会花费更多的时间，缓存层对用户来说是透明的。</p><p>在进行写操作时，缓冲的内容首先被写进PageCache中，这意味着数据并不是直接到达硬盘的，实际的硬件写入是在内核决定对<em>dirtypage</em>执行一次<em>writeback</em>时进行的。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202302242324979.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><h2 id="page-cache">Page Cache</h2><p>页缓存存储了被访问的文件中最有可能在最近的时间内被访问的片段，当操作硬盘文件时，<em>read()</em> and <em>write()</em>调用不是直接访问硬盘而是通过Page Cache访问</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202302242328238.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><p>执行读取操作时，首先查询页面缓存。如果数据已经加载到页面缓存中，就只用直接复制出来：不执行磁盘访问并且完全从内存读取。否则，文件内容将加载到页面缓存中，然后返回给用户。如果页面缓存已满，则最近最少使用的页面将刷新到磁盘上并从缓存中逐出以释放空间用于新页面。</p><p><em>write ()</em>调用只是将用户空间缓冲区复制到内核 PageCache，将写入的页面标记为<em>dirty</em>的。然后，内核在磁盘上写入修改，这个过程称为刷新或写回。实际的硬盘IO 通常不会立即发生。同时，read ()将从 Page Cache读取数据，而不是读取(现在已经过时的)磁盘内容。PageCache在读和写时都被使用到。</p><p>标记为<em>dirty</em>的页将刷新到磁盘，因为它们在缓存上的内容现在不同于在磁盘上的内容。这个过程称为回写。Writeback可能有潜在的缺点，比如会使得 IO请求拥挤，因此在使用它时，有必要了解用于写回的阈值和比率，并检查队列深度，以确保可以避免节流和高延迟。可以在<a href="https://www.kernel.org/doc/Documentation/sysctl/vm.txt">Linux内核文档</a>中找到更多关于调优虚拟内存的信息。</p><p>PageCache背后的逻辑可以用时间局部性原理来解释，即最近访问的页面将在最近的将来的某个时间点再次被访问。</p><p>另一个原则，空间局部性，意味着物理上临近的元素更有可能被连续访问。这个原则被用在一个称为“预取”的过程中，该过程提前加载文件内容，预期它们的访问并摊销一些IO 成本。</p><p>Page Cache还会通过延迟写操作和合并相邻读操作来提高 IO 性能。</p><p>消除歧义: 缓冲区缓存和页面缓存: 以前是完全独立的概念，而在2.4 Linux内核中得到统一。现在它主要被称为 Page Cache，但是一些人仍然使用术语Buffer Cache，它成为同义词。</p><p>Page Cache根据访问模式保存最近访问过的或者可能很快就会访问的文件块(预取或者用fadvise 标记)。由于所有 IO 操作都是通过 PageCache进行的，因此诸如读-写-读之类的操作序列可以从内存中提供，而不需要后续的磁盘访问。</p><p>当执行由内核或库缓冲区支持的写入时，确保数据实际到达磁盘（落盘）很重要，因为它可能在某处缓冲或缓存。当数据刷新到磁盘时会出现错误，这可能是在fsyncing 或关闭文件时。如果您想了解更多信息，请查看 LWN 的文章<ahref="https://lwn.net/Articles/457667/">确保数据到达磁盘</a>。</p><h2 id="direct-io">Direct IO</h2><p>在某些情况下，不需要使用内核页面缓存来执行IO。在这种情况下，可以在打开文件时使用 <em>O_DIRECT</em>标志。它指示操作系统绕过页面缓存，避免存储额外的数据副本，并直接对块设备执行IO操作。这意味着缓冲区直接刷新到磁盘上，而不需要首先将其内容复制到相应的缓存页面，然后等待内核触发写回操作。</p><p>对于使用 Direct IO的“传统”应用程序来说，很可能会导致性能下降，而不是加速，但是在正确的情况下，它可以帮助获得对IO 操作的细粒度控制，并提高性能。通常，使用这种 IO类型的应用程序实现它们自己的特定于应用程序的缓存层。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202302242345628.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><p>内核开发人员通常不赞成使用 DirectIO。到目前为止，Linux 手册页引用了Linus Torwalds 的话:</p><blockquote><p><a href="http://yarchive.net/comp/linux/o_direct.html">The thing thathas always disturbed me about O_DIRECT is that the whole interface isjust stupid</a>”.</p></blockquote><p>但是，PostgreSQL 和 MySQL 等数据库使用 Direct IO是有原因的。开发人员可以确保对数据访问的细粒度控制，可以使用自定义 IO调度器和特定于应用程序的缓冲区缓存。例如，PostgreSQL 使用 Direct IO ForWAL (write-ahead-log)，因为它们必须在确保持久性的同时尽可能快地执行写操作，并且可以使用这种优化，因为它们知道数据不会立即被重用，所以绕过Page Cache 编写数据不会导致性能下降。</p><p>不建议使用 Direct IO 和 Page Cache 同时打开同一个文件，因为即使数据在Page Cache中，也会对磁盘设备执行直接操作，这可能会导致不希望看到的结果。</p><h2 id="block-alignment">Block Alignment</h2><p>由于Direct IO涉及直接访问后备存储，绕过页面缓存中的中间缓冲区，因此要求所有操作都与扇区边界对齐。</p><p>换句话说，每个操作的起始偏移量必须是512的倍数，缓冲区大小也必须是512的倍数。在使用Page Cache 时，因为写首先进入内存，对齐并不重要:当执行实际的块设备写操作时，内核将确保将页分割成合适大小的部分，并对硬件执行对齐的写操作。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202302242348560.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202302242349187.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><p>例如，RocksDB通过前期检查确保操作是块对齐的(旧版本通过在后台对齐允许不对齐访问)。</p><p>无论是否使用 <em>O_DIRECT</em>标志，确保读和写都是块对齐的总是一个好主意。跨越段边界将导致多个扇区从磁盘加载(或写回)，如上图所示。使用块大小或恰好适合块内部的值可以保证块对齐的 I/O请求，并防止内核中的无用工作。</p><h2 id="memory-mapping">Memory Mapping</h2><p>内存映射(mmap)允许你访问一个文件，就好像它完全在内存中一样。它简化了文件访问，并经常被数据库和应用程序开发人员使用。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202302251418239.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><p>使用mmap，文件可以私有地映射到内存段，也可以在共享模式下映射到内存段。私有映射允许从文件读取，但是任何写操作都会触发页面的写时复制(copy-on-write)，以保持原始页面的完整性并保持更改的私有性，所以任何更改都不会反映在文件本身上。在共享模式下，文件映射与其他进程共享，这样它们就可以看到映射内存段的更新。此外，更改会传递到底层文件(需要使用msync 进行精确控制)。</p><p>除非另有说明，否则文件内容不会立即加载到内存中，而是以惰性方式加载。内存映射所需的空间已保留，但不会立即分配。第一次读取或写入操作会导致PageFault，从而触发相应页面的分配。通过传递 MAP_POPULATE标记，可以预先对映射区域进行处理并强制提前读取文件。</p><p>这种惰性加载的方式在标准IO中同样也会被用到，因为标准IO也是通过PageCache进行的，通常被称为<em>demandpaging</em>。在第一次内存访问期间，将发出一个 PageFault，它向内核发出信号，表明请求的页当前没有加载到内存中，因此必须加载。内核识别哪些数据必须从哪里加载。PageFault对于开发人员来说是透明的:程序流将继续进行，就像什么都没有发生一样。有时，页面错误可能会对性能产生负面影响</p><p>还可以使用保护标志(例如，只读模式)将文件映射到内存中。如果对映射内存段的操作违反了保护规则，就会引发SegmentationFault。</p><p>Mmap 是使用 IO 的一个非常有用的工具:它避免在内存中创建多余的缓冲区副本(不像标准IO，在进行系统调用之前，数据必须复制到用户空间缓冲区中)。此外，它还避免了触发实际IO 操作的系统调用(以及随后的上下文切换)开销，除非出现PageFault。从开发人员的角度来看，使用映射文件发出随机读操作看起来就像一个正常的指针操作，并且不涉及lseek 调用。</p><p>大多数时候提到的 mmap 的缺点在现代硬件的工作下已经关系不大了：</p><ul><li>Mmap 增加了管理内存映射所需的内核数据结构的开销:在今天的实际情况和内存大小中，这个参数不起主要作用。</li><li>内存映射文件大小限制:大多数情况下，内核代码对内存更加友好，并且64位架构允许映射更大的文件。</li></ul><p>当然，这也不是说任何时候共享内存文件都是合适的。</p><p>数据库引擎设计人员经常使用 mmap。例如，MongoDB 默认存储引擎是 mmap支持的，而 SQLite 广泛使用内存映射。</p><p>从目前来看，使用标准IO似乎简化了很多问题，并且有很多好处，但代价是失去了完全的控制：你在内核和pagecache的帮助下工作的很好。在某些情况下，这确实是真的。通常，内核可以使用内部统计信息更好地预测何时执行回写和预取页面。但是，有时可以从应用程序的角度考虑，来帮助内核以对应用程序有利的方式管理页面缓存。</p><p>将您的意图通知内核的一种方法是使用<em>fadvise</em>。使用这个标志，可以向内核说明你的意图，并让它优化页面缓存的使用:</p><ul><li><em>FADV_SEQUENTIAL</em> 指明该文件是按顺序读取的，从较低的偏移量读取到较高的偏移量，因此内核可以确保在实际读取发生之前提前获取页面。</li><li><em>FADV_RANDOM</em> 禁用预先读取，并且从页面缓存中清除不太可能在短期内访问的页面。</li><li><em>FADV_WILLNEED</em> 通知操作系统在不久的将来该进程将需要该页。这为内核提供了提前缓存页面的机会，并且在进行读操作时，可以通过PageCache而不是Page Fault提供服务。</li><li><em>FADV_DONTNEED</em> 建议内核可以为相应的页释放缓存(确保数据事先与磁盘同步的情况下)。</li><li>还有一些标志 (例如_FADV_NOREUSE_)，但是在Linux上它不起作用</li></ul><p>就像这个标记的名字表达的那样，它只是建议，内核并不能保证一定会像建议的那样执行。</p><p>由于数据库开发人员通常可以预测访问，因此 <em>fadvise</em>是一个非常有用的工具。例如，RocksDB 根据文件类型(SSTable 或HintFile)、模式(Random 或 Sequential)和操作(Write 或Compaction)将访问模式通知给内核。</p><p>另一个有用的调用是<em>mlock</em>。它允许您强制将页面保存在内存中。这意味着一旦页面加载到内存中，所有后续操作都将从页面缓存中提供。它必须谨慎使用，因为在每个页面上调用它只会耗尽系统资源。</p><h2 id="aio">AIO</h2><p>最后来谈一谈<ahref="https://github.com/torvalds/linux/blob/fbf4432ff71b7a25bef993a5312906946d27f446/fs/aio.c">LinuxAsynchronous IO</a>（AIO），AIO 是一个接口，允许启动多个 IO操作并注册在完成时触发的回调。操作将异步执行(例如，系统调用将立即返回)。在处理提交的IO 作业时，使用异步 IO 可以帮助应用程序在主线程上继续工作。</p><p>负责 Linux AIO 的两个主要系统调用是 <em>io_submit</em> 和<em>io_getevents</em>。<em>Io_mit</em>允许传递一个或多个命令，保存一个缓冲区、偏移量和必须执行的操作。可以使用<em>io_getevents</em>查询完成，该调用允许为相应的命令收集结果事件。允许使用一个完全异步的接口来处理IO、流水线 IO操作和释放应用程序线程，可能会减少上下文切换和唤醒的数量。</p><p>不幸的是，Linux AIO 有几个缺点: glibc 没有公开系统调用API，需要一个库来连接它们(libaio似乎是最流行的)。尽管多次尝试解决这个问题，但是只支持带有_O_DIRECT_标志的文件描述符，因此缓冲的异步操作不会起作用。此外，一些操作，例如stat、 fsync、 open 和其他一些操作并不是完全异步的。</p><p>值得一提的是，Linux AIO 不应该与 Posix AIO 混淆，Posix AIO完全是另外一回事。Linux 上的 Posix AIO实现完全在用户空间中实现，根本不使用这个特定Linux子系统。</p><h2 id="vectored-io">Vectored IO</h2><p>还有一个可能不是特别有名的方法来执行IO，VectoredIO（也同样被称为Scatter/Gather）。之所以这样称呼它，是因为它在缓冲区vector上运行，并允许在每次系统调用中使用多个缓冲区从磁盘读取和写入数据。</p><p>当执行向量读取时，字节将首先从数据源读入缓冲区(直到第一个缓冲区的长度偏移量)。然后，从第一个缓冲区的长度开始到第二个缓冲区的长度偏移量的字节将被读入第二个缓冲区，以此类推，就好像数据源正在一个接一个地填充缓冲区(尽管操作顺序和并行性不是确定的)。向量写操作的方式与此类似:缓冲区就像它们在写之前被连接起来那样执行写操作。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/jiengup/ImageBed@main/picgo202302251501927.png"alt="image.png" /><figcaption aria-hidden="true">image.png</figcaption></figure><p>这种方法可以通过允许读取较小的块(从而避免为连续块分配大内存区域)来提供帮助，同时减少用磁盘数据填充所有这些缓冲区所需的系统调用量。另一个优点是读和写都是原子的:内核防止其他进程在读和写操作期间在同一描述符上执行IO，从而保证数据的完整性。</p><p>从开发的角度来看，如果数据以某种方式布局在文件中(比如说，它被拆分成一个固定大小的头和多个固定大小的块)，就有可能发出一个调用来填充分配给这些部分的单独缓冲区。</p><p>这听起来相当有用，但不知何故，只有少数数据库使用向量IO。这可能是因为通用数据库同时处理多个文件，试图保证每个正在运行的操作的活性并减少它们的延迟，因此可以按块访问和缓存数据。向量IO对于分析工作负载和/或柱状数据库更有用，其中数据连续地存储在磁盘上，其处理可以在稀疏块中并行完成。其中一个例子是<ahref="https://github.com/apache/arrow/blob/master/java/memory/src/main/java/io/netty/buffer/ArrowBuf.java#L26-L27">ApacheArrow</a>.</p>]]></content:encoded>
      
      
      <category domain="https://jiengup.github.io/categories/Block-Storage/">Block Storage</category>
      
      
      <category domain="https://jiengup.github.io/tags/disk/">disk</category>
      
      <category domain="https://jiengup.github.io/tags/io/">io</category>
      
      <category domain="https://jiengup.github.io/tags/linux/">linux</category>
      
      
      <comments>https://jiengup.github.io/2023/02/25/15/38/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>用通俗易懂的语言介绍分布式系统中的CAP理论</title>
      <link>https://jiengup.github.io/2023/02/17/11/51/</link>
      <guid>https://jiengup.github.io/2023/02/17/11/51/</guid>
      <pubDate>Fri, 17 Feb 2023 03:51:35 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;译自&lt;a
href=&quot;http://ksat.me/a-plain-english-introduction-to-cap-theorem&quot;&gt;A
plain english introduction to CAP Theorem&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可能你会经常听说在设计</description>
        
      
      
      
      <content:encoded><![CDATA[<p>译自<ahref="http://ksat.me/a-plain-english-introduction-to-cap-theorem">Aplain english introduction to CAP Theorem</a></p><p>可能你会经常听说在设计分布式系统的时候，有一套CAP理论限制，今天我们就试着通过一个现实中的解决方案来理解所谓的CAP。</p><h2 id="chapter-1-记忆存储有限公司你的全新创业之旅">Chapter 1:“记忆存储有限公司”——你的全新创业之旅</h2><p>昨晚，你多年的配偶由于收到了你为他准备的生日礼物，对你还记得她的生日表示了极大的欣赏与感激，这使得你催生了一个奇怪的想法。人们总是不善于记住事情，但是你却对此非常擅长，所以为什么你不能用你的天赋来进行一次创业呢？你越想越觉得喜欢这个想法，甚至在你脑海中已经形成了发布在报纸上的广告语。</p><blockquote><p>记忆存储公司！—— 完全不记也绝不会忘！</p><p>你曾有过因为忘记事情而感到悲伤吗？别担心，只需要一通电话，就可以解决这样的烦恼。</p><p>当你需要去记住一些事情的时候，只需要拨打135555REMEM然后告诉我们你需要记住什么。例如说，打给我们然后让我们记住你老板的电话，然后忘记它。当你需要知道它的时候，你只要再通过一样的电话135555REMEM打回来然后我们会告诉你你老板的电话是多少。</p><p>收费：每次服务仅需0.1元</p></blockquote><p>因此，你通常的电话交流会类似下面这样：</p><ul><li>顾客：嘿，你可以帮我记下我邻居的生日吗？</li><li>你：当然，他的生日是什么时候。</li><li>顾客：1月2日</li><li>你：（在你的笔记本上该顾客对应的那一页写下这个日子）存好了，在你再次需要知道你邻居生日的时候打给我们。</li><li>顾客：非常感谢！</li><li>你：没关系，我们会从你的信用卡上扣除0.1元。</li></ul><h2 id="chapter-2-公司业务增长">Chapter 2: 公司业务增长</h2><p>好消息，你的创业项目被YCombinator赞助了。你的想法太过于简单，除了一个电话和一个纸质笔记本之外再不需要别的东西，当你的业务像火一样蔓延开来的时候，你开始一天要处理上百个电话服务。</p><p>然后问题就出现了，你发现越来越多的客户需要在和你通电话之前排队，他们其中的大多数甚至因为等待的时间太长而挂掉了电话。除此之外，当你在某一天由于生病而无法工作时，你就丢失了一整天的生意，并且想想在那天需要从你这里获取信息的顾客有多么失望。</p><p>你决定是时候扩展你的业务能力了，于是你找来了你的妻子帮你。</p><p>你们商量了一个简单的计划：</p><ol type="1"><li>你和你的妻子各有一部手机</li><li>顾客仍然拨打135555REMEM，但需要额外记住一个数字</li><li>一个pbx会将一个顾客的拨打平等的路由到你们其中空闲的人</li></ol><h2 id="chapter-3-创业路上的第一道坎">Chapter 3:创业路上的“第一道坎”</h2><p>在你运作你的新工作模式2天之后，你从你的忠实客户Jhon那接到一通电话，通话如下：</p><ul><li>Jhon：嘿！</li><li>你：非常荣幸接听您的来电，您需要什么帮助？</li><li>Jhon：你可以告诉我我前往新德里的航班是什么时候吗？</li><li>你：当然，请稍等 （你开始在你的笔记本中查找）（噢！在Jhon的页面中没有关于航班的条目信息）</li><li>你：先生，您可能搞错了，您从来没有告诉过我们您到新德里的航班信息</li><li>John：什么！我昨天明明给你们公司的员工通过电话！（愤怒的挂掉电话）</li></ul><p>为什么这种情况会发生呢？有可能是John撒谎了吗？你思考了一会心中有了答案！有可能是John昨天打给了你的妻子？你跑到妻子的办公桌前然后检查她的笔记本，发现John的那条记录果然记录在上面。你将这个事故告诉了你的妻子，她也意识到了问题的严重性。</p><p>显然你设计的分布式工作模式中存在严重的缺陷！</p><p><strong>你的分布式系统是不一致（notConsistent）的！有这样的情况存在：当一个顾客告诉了你或者你妻子一些事情，但是当他再次打过来的时候却被路由到了另一个人，因此在目前的记忆存储公司中产生不了一致性的回复。</strong></p><h2 id="chapter-4-修复一致性的问题">Chapter 4: 修复一致性的问题</h2><p>那么，你的竞争对手可能会忽略一次糟糕的服务，但是你不会，你想了这个问题一整晚，然后在早上得出了一个漂亮的计划，你把妻子叫醒然后告诉了她：</p><p>“亲爱的，从现在开始我们要这样做”</p><ul><li>每当我们其中一个人接到一通电话客户要求我们记住或者更新某些事物（写）的时候，我们在挂掉电话之前要告诉彼此通话的内容</li><li>通过这种方式，我们两个人都需要将获得的信息记录到笔记本上</li><li>当有一个询问信息的通话（客户打电话过来请求我们告诉他他存储在我们这里的某些信息）（读）的时候，我们不再需要向彼此获取信息，因为我们两都有了最新的信息记录在笔记本上，我们可以直接回复客户</li></ul><p>”这个计划很完美，虽然存在一个问题“你说，“那就是一次更新请求需要我们两人同时处理，这样我们就不能并行工作了。例如当你接到一个更新请求并且告诉我也要更新的时候（写），我不能接听其他的电话。但是由于我们接到的电话大多数都是询问电话（读），因为客户总是告诉我们某些信息一次，但是询问它很多次，所以目前这个方案也还可行。另外，我们无论付出什么代价也千万不能告诉彼此错误的信息。“</p><p>“非常干净利落的方案！“，你的妻子说，”但是你还是漏掉了这样的系统中仍然存在的一个缺陷。如果我们两个人其中某一个在某一个特殊的日子不工作怎么办？这样的话，我们就不能够接听任何更新信息的电话。因为我们中有一个人的信息没办法更新！</p><p><strong>我们的系统存在可用性(Availability)问题，例如，在这种情况下，如果一个更新通话打进来，尽管我已经在我的笔记本上记录下来了这条信息，我永远也不能挂断电话，因为我在那天无法使得你也同步更新这条信息！</strong></p><h2 id="chapter-5-有史以来最完美的计划">Chapter 5:有史以来最完美的计划？</h2><p>你开始慢慢意识到你的分布式系统可能没有你一开始想的那么简单。难道想出一个同时兼具<strong>一致性和可用性</strong>的方案就这么难吗？这对其他人来说可能很难，但是对你来说并不！接着第二天清晨你又想到了一个方案，而你的竞争者却在睡大觉，梦里可没有这样的方案！你又一次早早的叫醒了妻子..</p><p>"看"，你告诉她，“我们这样做就能同时保证一致性和可用性。“新的方案和昨天我告诉你的非常相似：</p><ul><li>i）每当我们其中一个人收到一通更新电话时，在挂掉电话之前，如果另外一个人今天在岗，我们就会把这通电话的内容告诉另外一个人，这种情况下我们两人都会在笔记本上记录下更新</li><li>ii）但是如果另外一个人今天并不在岗，我们就需要向另外那个人发送一条包含更新内容的邮件</li><li>iii）第二天当另外那个人休假结束回到岗位上的时候，在他开始今天的正式工作之前，他首先需要过一遍所有的邮件，<strong>按照顺序更新他的笔记本记录</strong></li></ul><p>“你真是个天才！”，妻子这样说，“我再找不出这个系统中的其他缺陷了，就让我们按照这套方案运行吧！记忆存储公司，现在终于<strong>兼备了一致性与可用性！</strong>”</p><h2 id="chapter-6-妻子生气了">Chapter 6: 妻子生气了！！！</h2><p>新方案实施后的一段时间以内，一切都是那么顺利，你们的系统是一致的，甚至在其中一个人没有办法工作的时候也能正常运行。但是如果当你们两个人都工作的时候，你们其中有一个人不向另外一个人更新信息怎么办呢？想一想那些天你老早就把妻子吵醒，然后告诉她你所谓的充满漏洞的“完美计划”，如果这使得在某一天你的妻子虽然愿意接听电话，但是对你太过于生气，不愿意与你交谈并且同步更新怎么办？你的想法通通失效了！你的方案目前来看虽然是一致的和可用的，但是却不是<strong>分区容忍（PartitionTolerant）</strong>的！</p><p>你可以通过今天不接听任何电话而是哄妻子开心来实现解决分区容忍的问题..但是这样的话你的系统又没有办法正常运行了..</p><h2 id="chapter-7-总结一下吧">Chapter 7: 总结一下吧～</h2><p>现在我们再来看CAP理论，他是这样说的，当你设计一个分布式系统的时候，你往往不能同时实现<strong>一致性（Consistency），可用性（Availability）以及分区容忍性（Partitiontolerance）</strong>三者，通常只能保证同时满足其中的两个性质。</p><ul><li>一致性（Consistency）：你的顾客，每当他们向你们更新信息的时候，他们总是能够在随后打回来的时候获取到<strong>最新的更新信息</strong>，无论他们打电话的间隔有多短</li><li>可用性（Availability）：尽管你们其中一个人无法工作，记忆存储公司也还是能够提供服务</li><li>分区容忍性（Partitiontolerance）：记忆存储公司仍然能够工作，甚至当你和妻子之间丢失信息时候</li></ul><h2 id="bonus-通过招聘新员工达成最终一致性">Bonus:通过招聘新员工达成最终一致性</h2><p>有另外一个值得深思的地方，你可以招聘一个秘书，他负责当你们其中一个人的笔记有更加新的信息的时候，更新其他人的笔记本。这样带来的巨大好处是，他可以在后台工作，因此你或者你妻子的更新操作不会由于等待对方的更新而被“阻塞”。这是很多NoSql系统的工作方式，一个结点在本地对自己更新之后，一个后台的进程将会逐步同步其他结点的更新，最终达到一致性。唯一的问题是，你们可能会在某些时候丢失一致性，例如，一个顾客的更新信息首先给到了你的妻子，在秘书在有机会对你的笔记本进行更新之前，顾客打回来电话并且被路由到了你这里，这样他就无法获得一个一致性的回复。但是如果这种情况并不会产生多大的影响，这就是一个不错的方案，比如说假设顾客不会那么快（例如5分钟之内）忘记事情然后打回来。</p><p>这就是CAP理论和最终一致性，通过大白话讲给你听。</p>]]></content:encoded>
      
      
      <category domain="https://jiengup.github.io/categories/Distributed-System/">Distributed System</category>
      
      
      <category domain="https://jiengup.github.io/tags/distributed/">distributed</category>
      
      <category domain="https://jiengup.github.io/tags/CAP/">CAP</category>
      
      <category domain="https://jiengup.github.io/tags/consistency/">consistency</category>
      
      <category domain="https://jiengup.github.io/tags/availability/">availability</category>
      
      
      <comments>https://jiengup.github.io/2023/02/17/11/51/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【文献精读】Sinan: ML-Based and QoS-Aware Resource Management for Cloud Microservices</title>
      <link>https://jiengup.github.io/2022/11/02/20/21/</link>
      <guid>https://jiengup.github.io/2022/11/02/20/21/</guid>
      <pubDate>Wed, 02 Nov 2022 12:21:38 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;随着云应用逐渐从大型集成的整块向形成大量的、松散耦合的微服务构成转化，其中除了一系列的好处之外，同时也产生了一些问题。例如微服务中的依赖关系使得资源管理更复杂化了，因为它们会引发背压效应，加剧QoS违背。&lt;br</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="abstract">Abstract</h2><p>随着云应用逐渐从大型集成的整块向形成大量的、松散耦合的微服务构成转化，其中除了一系列的好处之外，同时也产生了一些问题。例如微服务中的依赖关系使得资源管理更复杂化了，因为它们会引发背压效应，加剧QoS违背。<br />本文提出了Sinan，一个由数据驱动的、在线的、关注QoS的交互式云微服务集群管理系统。Sinan使用了一系列可扩展的、经过验证的机器学习模型，以优化端到端的尾延时为目的出发，使用机器学习模型决定依赖间的性能影响以及为每一层分配合适的资源。本文同时在专用的本地集群以及GoogleComputeEngine上部署的端到端应用数据上进行了验证，Sinan在保持高的资源利用率的情况下，同时能够很好的保证QoS。<br />此外，Sinan的技术是可以解释的，这意味着云运营商可以从机器学习模型中获得如何更好地部署和设计应用程序以减少不良性能情况的出现。</p><h2 id="overview">Overview</h2><h3 id="问题构造">问题构造</h3><p>目前的微服务框架是由多个具有依赖性的微服务构成的土结构，其中每种微服务都有着不同的资源需求、伸缩需求以及副本状态。Sinan关注与以尾延时作为QoS限制的复杂的、交互性的微服务中的资源管理。<br />大多数集群资源管理模块主要关注CPU和内存，而微服务通常为“无状态”的，因此Sinan优先考虑分配CPU资源，同时在单核和多核粒度通过DockerAPI控制Linux cgroups来实现。</p><h3 id="研究的应用数据集之一">研究的应用（数据集之一）</h3><p>用了两个端到端的交互式应用：一个酒店预定服务，一个社交网络</p><h4 id="酒店预定服务">酒店预定服务</h4><p>服务支持利用地理位置搜索酒店、预定以及推荐。其由Go实现，层与层之间通过gRPC交流。数据端使用memcached作为内存缓存，MongoDB作用作持久化。</p><h4 id="社交网络">社交网络</h4><p>用户可以创建带有文字、多媒体、链接以及提及他人的推文，随后该推文会被广播到用户所有的粉丝。图片会通过一个图片过滤器，文字也会通过一个文字过滤器，违反用户守则的内容会被丢弃。用户可以根据他们的时间线阅读推文，我们使用Read98社交网络来组织用户数据。用户活动遵循Twitter 的用户行为，文本长度分布模拟 Twitter的文本长度分布（这两个模拟有研究支撑）。</p><h3 id="挑战和机器学习任务的需求">挑战和机器学习任务的需求</h3><ol type="1"><li>不同层级服务之间的依赖关系：微服务之间的依赖关系并不是完美的流水线形式，因此会产生背压效应，而且还难以监测和预防。因此资源调度器需要有微服务的全局视角以及可以预测依赖带来的端到端的性能影响。<br /></li><li>系统的复杂性：这意味着需要在以一个特定规则对于每一个微服务形成的资源分配策略的空间中搜索。之前的方法有用资源利用率或延时来指导分配策略的，队列的方法则是使用队列长度来刻画系统状态。但是这些方法在有着大量依赖关系的复杂的微服务系统中都不可使用。<br /></li><li>队列延迟效应：当资源减少时，QoS并不会很快被违反，因为队列还有一些长度，有堆积的空间，需要时间产生堆积。反之亦然，当QoS被触发时，就算及时采取了增加资源的行动，堆积的队列也需要一定时间去消耗。因此我们需要机器学习的方法来根据长期的资源分配特征进行资源分配，避免资源减少的太过于迅速，而且需要能够防御性的进行资源增加（蓝线）</li><li>资源分配空间的边界：对于资源管理模块来说，快速确定资源分配的边界以满足QoS的同时分配尽量少的资源是非常重要的，但是现有的方法要不是在巨大的空间中随机探索，要不是根据历史数据进行学习，这两种方法往往不能找到特别靠近边界的解。随机探索是盲目的，而历史数据中往往会多分配资源，由此产生了一定的偏差。</li></ol><h2 id="提出的方法">提出的方法</h2><p>Sinan使用了一个机器学习模型来学习端到端性能的依赖特征，以及执行分配决策。同时还设计了一个高效的空间搜索算法以搜索资源分配空间，特别是可能会引发QoS违反的边界区域。</p><p>Sinan的机器学习模型可以预测端到端的延时以及给定一个资源分配情况下，通过系统状态和历史信息预测QoS违背的概率。<br />从宏观上来看，Sinan的工作流可以被分为：</p><ol type="1"><li>数据收集模块使用一个精细设计的搜索算法来收集训练数据（解决挑战4）<br /></li><li>通过收集到的数据，Sinan训练了两个机器学习模型：一个卷积神经网络（CNN），一个bostedtrees（BT），CNN通过预测未来一段时间的端到端的尾延时解决挑战1和2，BT则解决挑战3，预测未来发生QoS侵犯的概率，来解释系统在队列方面的特征。<br /></li><li>运行过程中，Sinan计算瞬时的尾延时以及即将可能发生的QoS违反，然后根据QoS限制来调整资源分配。<br /></li><li>如果应用或者底层的系统在任何时间发生改变，Sinan将会重新训练对应的模型</li></ol><h3 id="机器学习模型">机器学习模型</h3><p>Sinan的目的是通过一个特定的资源分配情况准确的预测应用的性能。调度器可以对于每个微服务通过可能的资源分配情况查询模型，以选择一个最优的资源分配情况，以满足QoS。<br />挑战3指出，资源分配决策的影响要在一段时间之后才能发挥作用，因此需要训练一个神经网络来预测未来一段时间内的时延分布。但是经过实验发现，随着预测的未来更远，预测精度会急剧下降。因此预测仅仅是根据目前收集到的和过去历史的指标得出的，这些足够预测很近的未来，但是很难捕捉到未来一段时间后发生演变的微服务之间的依赖关系。</p><p>为了解决这个问题，我们设计了一个两阶段的模型，首先，利用一个卷积神经网络来预测下一个时间戳的端到端尾延时，接着利用一个BoostedTrees（BT）模型根据CNN抽取出来的特征来估计未来的QoS违背概率。BT有着相对更少的超参数，因此比较不容易发生overfitting。我们称CNN模型为短期延迟预测器，将BT模型称为长期违规预测器。<br />时延预测器<br />卷积神经网络需要同时学习到微服务之间的依赖关系与资源使用和应用性能的时序特征。因此，应用的拓扑结构和时序信息都要被编码成CNN的输入，它包含如下几个部分：</p><ol type="1"><li>一个三维的tensor，包含一个历史时间窗口内的每一层的资源使用情况<br /></li></ol><ul><li>ay维对应不同的微服务，连续的列对应连续的层<br /></li><li>bx维对应时序特征，一行对应一个时间戳<br /></li><li>cz维（通道数）对应不同的资源使用指标，包括CPU利用率、内存使用、以及网络使用等等，这些都直接通过Docker的cgroup接口得到。<br /></li></ul><ol start="2" type="1"><li>历史时间窗口内的端到端的延时分布矩阵（2维）<br /></li></ol><ul><li>ax维是T个时间戳<br /></li><li>by维是不同延时（95%-99%）的向量<br /></li></ul><ol start="3" type="1"><li>待测试的下一个时间戳的资源分配策略，同样也被编码成了一个矩阵<br /></li></ol><ul><li>ax维是N层<br /></li><li>by维是CPU限制</li></ul><p>损失函数的设计过程如下：<br />首先，使用了一个比较常见的均方差损失：<br /><span class="math display">\[\mathcal{L}(X, \hat{y},W)=\sum_{i}^{n}\left(\hat{y}_{i}-f_{W}\left(x_{i}\right)\right)^{2}\]</span></p><p>但是考虑到鉴于交互式微服务的飙升行为导致非常高的延迟，如果用上面的损失函数会造成在训练集上的过拟合现象，实际部署的时候就会导致延时预测高了。由于延时预测器目的是在满足尾延时要求的情况下找到最优的资源分配策略，因此损失函数可以更偏向满足QoS的训练样本，因此选择使用了一个缩放函数，在计算损失函数之前同时缩放预测的尾延时和实际的尾延时。<br /><span class="math display">\[\phi(x)=\left\{\begin{array}{ll}x &amp; x \leq t \\t+\frac{x-t}{1+\alpha(x-t)} &amp; x&gt;t\end{array}\right.\]</span></p><p>其中，延时范围为(0,t)，alpha是超参，可以根据不同的衰减策略进行调整。</p><h3 id="qos违背预测器">QoS违背预测器</h3><p>违背预测器解决一个二分类任务，预测一个给定的分配策略是否在未来会引发QoS违背。<br />出于计算消耗和减少过拟合的可能考虑，BT模型将利用CNN输出的Lf作为输入，同时还将k个时间戳的以同样的资源配置作为输入。<br />每一个树的叶子结点有一个分数，代表发生了违背或没有发生违背，将发生违背的所有分数加起来得到Sv，没有发生违背的所有分数加起来得到SNV，则发生违背的概率可以由softmax得到</p><h2 id="系统设计">系统设计</h2><h3 id="系统架构">系统架构</h3><p>当收到用户请求时，Sinan 通过 Docker 和 Jaeger收集资源和性能指标，将收集到的指标输入机器学习模型，并使用模型的输出来相应地为每一层分配资源。分配决策定期在线重新评估。<br />Sinan将1秒作为一个决策区间，周期性的给出决策，这和延时的QoS要求保持一致。<br />CentralizedScheduler向分布式的系统询问，以获得每一层的CPU、内存以及网络使用情况，除此之外，还向API网关询问用户负载统计信息。<br />每一层的所有副本的资源用量都会在送入模型之前取平均<br />根据模型的输出结果，Sinan 选择一个满足 QoS的分配向量，使用最少的资源，并将其决策传递给每个节点的代理执行。</p><h3 id="资源分配空间搜索">资源分配空间搜索</h3><p>对于每一个微服务的层，看作是多臂老虎机的一个手臂，则问题可以被建模成一个多臂老虎机问题。对于每一层，将其资源分配情况和端到端的延时映射成一个伯努利分布，定于信息获取值为对一层分配特定的资源总量，其对应的伯努利分布对于概率p的置信区间减少的期望。每一步，我们对于每一层选择采用其信息获取值最大的动作。</p><p><span class="math display">\[\begin{array}{c}o p_{T}^{s}=\arg \max _{o p} C_{o p}\cdot\left(\sqrt{\frac{p(1-p)}{n}}-p\sqrt{\frac{p_{+}\left(1-p_{+}\right)}{n+1}}\right. \\-(1-p) \sqrt{\left.\frac{p_{-}\left(1-p_{-}\right)}{n+1}\right)}\end{array}\]</span></p><p>通过最大化上式，算法被鼓励去寻找以最小资源量满足QoS的边界点，因为完全满足或者违背QoS的分配所得到的信息都几乎为0，该算法优先探索对QoS 影响不确定的资源分配，如 p = 0.5的资源分配。<br />为了精简操作空间，Sinan 对数据收集和在线调度都实施了一些规则*。</p><h2 id="实验和结果">实验和结果</h2><p>分别在local和Google Claster Engine上进行了试验<br />本地配置：<br />本地集群有4个80核的服务器，每个服务器有256GB 的RAM。我们分别为酒店预订和社交网络收集了31302和58499个样本，使用我们的数据收集过程，并将它们以9:1的比例分割成训练和验证集，然后进行随机重组。数据收集代理分别为社交网络和酒店预订运行16小时和8.7小时，收集更多的训练样本并不能进一步提高准确性。</p><p>可解释性*</p>]]></content:encoded>
      
      
      <category domain="https://jiengup.github.io/categories/Research/">Research</category>
      
      
      <category domain="https://jiengup.github.io/tags/QoS-Prediction/">QoS Prediction</category>
      
      <category domain="https://jiengup.github.io/tags/Machine-Learning/">Machine Learning</category>
      
      <category domain="https://jiengup.github.io/tags/Resource-Allocation/">Resource Allocation</category>
      
      
      <comments>https://jiengup.github.io/2022/11/02/20/21/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>基于深度学习方法的多维易变云服务负载的精准预测 【文献精读】Towards Accurate Prediction for High-Dimensional and Highly-Variable Cloud Workloads with Deep Learning</title>
      <link>https://jiengup.github.io/2022/10/25/11/18/</link>
      <guid>https://jiengup.github.io/2022/10/25/11/18/</guid>
      <pubDate>Tue, 25 Oct 2022 03:18:36 GMT</pubDate>
      
        
        
      <description>&lt;h1
id=&quot;基于深度学习方法的多维易变云服务负载的精准预测&quot;&gt;基于深度学习方法的多维易变云服务负载的精准预测&lt;/h1&gt;
&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;适应性强以及精准的云服务负载的预测对于云计算的资源分配是非常必要的。然而，现有的方法并不能够有效的预测多维度的</description>
        
      
      
      
      <content:encoded><![CDATA[<h1id="基于深度学习方法的多维易变云服务负载的精准预测">基于深度学习方法的多维易变云服务负载的精准预测</h1><h2 id="摘要">摘要</h2><p>适应性强以及精准的云服务负载的预测对于云计算的资源分配是非常必要的。然而，现有的方法并不能够有效的预测多维度的、不稳定的云服务负载，这往往导致了云服务资源的浪费以及违背对于用户的服务水平协议（SLAs）。循环神经网络（RNN）往往对于序列数据的分析非常有效，因此它近期被应用于解决负载预测问题。然而，RNN在学习长期依赖的表现十分不足，从而不能对于负载进行精确的预测。为了解决这个关键的问题，我们提出了一种基于深度学习的方法PredictionAlgorithm for cloudWorkloads（L-PAW）。首先，设计了一个上层的自动编码器（TSA）以从原始的高维度的负载数据中抽取负载间的潜在关系。接着，我们整合了TSA以及门循环单元（GRU）块形成一个循环神经网络以达到适应性强以及精确的对于易变的负载的预测。利用Google和阿里巴巴公司数据中心的真实数据在DUX-based的集群下进行了实验以对L-PAW进行实现以及实验。我们在不同类别的负载用变化的预测长度对L-PAW的适应性和有效性进行了验证。实验结果表明L-PAW对比于传统的RNN-based方法以及其他的负载预测的方法在多维易变的负载数据上达到了非常好的预测精度。</p><h2 id="引入">1. 引入</h2><p>作为非常普遍的范例计算范例之一的云计算为用户提供了需求可变的计算、存储以及网络资源。并且在用户和服务提供商之间保证了服务水平协议（SLAs）。当用户请求同时到达的时候，云服务负载会突发导致云资源不能有效的访问。相反的，云服务资源在一个低水平状态一直挂起也同样会导致资源的浪费。云服务负载的变化导致了资源的过度分配或者欠分配，从而导致了不必要的资源浪费或者违反SLA的现象。因此，云服务提供商必须能够迅速的变换资源分配的策略以保证SLA的同时提升云服务资源的利用率。为了达到这些目标，快速的以及适应性强的五仔预测方法对于云计算服务来说是非常必要的。通过对于未来负载情况的精准预测，可以通过提前配置和分配资源以更加有效的以及合理的分配。因此，云计算服务的负载预测面临以下的两大挑战：</p><ol type="1"><li>负载特征的高可变性。负载的特征（例如谷歌云数据中心的负载会随机变化[5]以及高相关性的周期性的DUX-based集群负载）在不同的时间粒度上（例如秒级和天级）一直在变化。根据对于阿里巴巴云数据中心的分析报告，他们的平均CPU利用率在一天之内能从5%到80%之间变化，且波动特别高。这种负载特征的易变特征使得精确有效的负载预测十分困难。</li><li>负载数据的高维度。云计算服务中的负载数据通常有着高维度的问题。例如，对于一个有着1000个工作机器的云数据中心需要讲一个1000维的负载数据作为预测模型的输入用作训练。这些高维度的数据有非常多的冗余信息以及噪声，这不仅对负载预测造成了更多误差，并且会导致预测模型造成更高的计算消耗。为了应对负载特征的高可变性，用户负载特征之间的相互关系必须被有效的捕捉到并且利用起来以得到一个准确的负载预测方法，这样才能使得算法可以使用各种各样的负载。以应对负载数据高维度的挑战，原属负载数据的特征需要进行进一步的分析以及抽取，减少负载数据的维度以及预测误差以得到更加有效准确的负载预测。</li></ol><p>有关负载预测的问题受到了研究者的大量关注，然而，许多经典的方法是基于回归理论、启发式或者传统的人工神经网络的，这些方法往往需要负载数据相对常规，或者具备清晰的趋势以达到精准预测。同时，传统的神经网络没有充分利用网络结构中神经元的相关性。因此，这些方法不能有效的实现对于高可变性的负载数据的精准预测。另外，大多数方法关注的都是一些小体量的活着高性能的计算机系统中的负载，这些情景下负载的变化情况相对大体量的云计算系统（例如云数据中心）更加简单。因此，这些算法也不能很好的适应具有高可变负载的真实的云计算环境，在这样的环境下它们的预测准确率会大打折扣。</p><p>由于RNN对于序列数据处理的优秀能力，它可以用于解决高可变负载的预测问题。然而，训练一个高效的RNN是一个潜在的挑战。因为存在提督小时的问题，传统的RNN不能有效的学习长存依赖。一些衍生的RNN，例如长短式记忆神经网络（LSTM）以及门循环单元（GRU），被提出具有强大的能力解决长存依赖的问题。特别的，对比于LSTM，GRU不进可以达到优秀的预测准确性，而且还有着更少的参数以使得训练过程更加高效。</p><p>然而，由于负载数据的高维特点，训练一个基于RNN的预测模型是一个非常费时的任务，具有庞大的计算复杂度。为了解决这个问题，一个可行的方法是通过抽取潜在的特征表达来减少负载数据的维度。一些方法可以有效的减少数据的维度例如主成分分析（PCA）以及自动编码器（auto-encoder）。然而，PCA依赖线性方法去寻找高维数据中最大偏差的方向，这限制了可减少维度的类型。相对的，基于自动编码器的方法（例如sparseauto-encoder）克服了这个限制。它引入了非线性的神经网络。但是传统的自动编码器常常过度使用了隐藏单元，这回导致在负载数据上减少维度变得十分低效。</p><p>为了解决负载预测中这些潜在的挑战，我们首先设计了一个顶部稀疏的自动编码器（top-sparseauto-encoder，TSA）以有效的减少负载数据的维度。接着，经过压缩的负载数据被当作输入喂入云负载基于深度学习的预测算法（L-PAW），为了增加适应性、实际应用价值以及对于高可变负载数据的有效预测，本篇研究的主要贡献总结如下：</p><ul><li>设计了一个顶部稀疏的自动编码器以有效的从原始数据中抽取低维度的潜在的特征表达，这使得负载数据可以被有效的压缩，而仅仅需要考虑的是通过高层的活跃度；奥选择隐藏单元的个数。</li><li>提出了一个高效的基于深度学习的预测算法用于预测云服务的负载数据。该算法可以通过整合TSA和GRU块的RNN中有效的学习历史负载数据中的长存依赖。L-PAW算法可以非常好的适应变化多端的负载数据并且通过利用GRU中设置的更新门和重置门来捕捉潜在的历史信息实现精确的负载预测。</li><li>利用真实生产环境下的负载数据进行了模拟实验，以验证提出的L-PAW算法在云负载预测上的有效性和适应性。结果表明L-PAW相对于传统的基于RNN的算法以及其他预测方法在高维度、高可变性的真实云负载数据上表现得更好。</li></ul><h2 id="预备工作">2. 预备工作</h2><p>在这个章节，我们讲简短的介绍稀疏自动编码器（SA）以及循环神经网络，基于这两个技术的基本原理，我们可以解决云服务负载上高维度高可变性的问题。</p><h3 id="稀疏自动编码器">2.1 稀疏自动编码器</h3><p>稀疏自动编码器可以从没有标签的数据中自动的学习潜在的特征，在实际应用中，这些被SA抽取出来的特征表达可以被用于替换原始的数据，这常常能够让人工神经网络的学习过程表现地更好。更重要的是，SA仅仅用了一层隐藏层来构造，这会使得输出数据和输入数据尽量相似。并且，隐藏层必须满足特定的稀疏度，这也意味着隐藏层不能够携带过多的信息。因此，输入数据会被压缩到隐藏层里，然后在输出层解压。</p><p>SA学习一个函数 <span class="math inline">\(y_{i}=f\left(Wx_{i}+b\right) \approx x_{i}\)</span> ，其中<spanclass="math inline">\(x_i\)</span>以及<spanclass="math inline">\(y_i\)</span>是属于n维的实数集。换句话说，SA的目的是拟合一个函数使得输出<spanclass="math inline">\(y_i\)</span>可以和输入<spanclass="math inline">\(x_i\)</span>足够的接近。在这个过程中，一些重要的特征会从输入数据中被抽取出来。另外，输入数据可以被隐藏层替换（例如，隐藏层的神经元），以此来达到数据压缩的作用。尽管隐藏层单元的个数可能非常大，输入数据的潜在特征仍然可以通过加入稀疏约束被找到。特别的如果<spanclass="math inline">\(a^{(h)}_j\)</span>用来表示隐藏单元<spanclass="math inline">\(j\)</span>的激活度，那么它的平均值<spanclass="math inline">\(\hat{\rho}_j\)</span>，可以按照以下的方式计算：</p><p><span class="math display">\[\hat{\rho}_j=\frac{1}{n}\sum_{i=1}^{n}\left[a^{(h)}_j\left(x_i\right)\right]\]</span></p><p><span class="math inline">\(\hat{\rho}_j\)</span>被强制拟合为<spanclass="math inline">\(\rho\)</span>，其中<spanclass="math inline">\(\rho\)</span>是稀疏度参数，应该趋近于0以满足隐藏层的稀疏度限制。因此，下述的这个项作为当<spanclass="math inline">\(\hat{\rho}_j\)</span>严重偏离<spanclass="math inline">\(\rho\)</span>时候的惩罚项：</p><p><span class="math display">\[\sum_{j=1}^{N_h}\rho\ln{\frac{\rho}{\hat{\rho}_j}}+(1-\rho)\ln{\frac{1-\rho}{1-\hat{\rho}_j}}\]</span></p><p>其中<spanclass="math inline">\(N_h\)</span>是隐藏单元的数目，上述的公式同样也可以根据Kullback-Leibler(KL) divergence以<span class="math inline">\(\textstyle\sum_{j=1}^{N_h}KL(\rho||\hat{\rho}_j)\)</span>给出</p><p>作为衡量两个特定分布差异登记的标准函数，当<spanclass="math inline">\(\hat{\rho}_j\)</span>与<spanclass="math inline">\(\rho\)</span>足够接近时，能够达到最小KL差异，这也意味着最小化惩罚项的过程对于<spanclass="math inline">\(\hat{\rho}_j\)</span>与<spanclass="math inline">\(\rho\)</span>的近似同样有作用。因此，SA整体的损失函数，<spanclass="math inline">\(J_{sparse}(W,b)\)</span>，按照如下方式定义：</p><p><span class="math display">\[J_{sparse}(W, b) = J(W, b) + \beta\sum_{j=1}^{N_h}KL(\rho||\hat{\rho}_j)\]</span></p><p>其中<span class="math inline">\(J(W,b)\)</span>时神经网络的损失函数，<spanclass="math inline">\(\beta\)</span>是控制稀疏度惩罚项的权重。</p><h3 id="循环神经网络">2.2 循环神经网络</h3><p>循环神经网络强调了隐藏层神经元之间的联系，这可以利用历史数据解决序列问题。特别的，传统神经网络中的隐藏层是全连接或者部分连接的，但是不同神经网络间的神经元是不联系的。相反的，RNN的目的是利用一个序列去构造历史数据与目前状态的关系。因此，RNN中的隐藏层之间的神经元是有连接的。这也意味着隐藏层的输入不仅仅只有当前时刻的输入层数据，同时也包括前面时刻的隐藏层的输出数据。典型的RNN的结构如图1所示，RNN的链接属性同样也揭示了与序列过程的潜在联系。</p><p>基于历史数据和目前的输入，未来的输出可以被按照以下的过程预测：</p><p><span class="math display">\[s_{t}=\tanh \left(U \cdot x_{t}+W \cdot s_{t-1}\right),y_{t}=\operatorname{softmax}\left(V \cdot s_{t}\right)\]</span></p><p>其中<span class="math inline">\(s_t,x_t\)</span>以及<spanclass="math inline">\(y_t\)</span>表示了隐藏层的状态，输入以及t时刻的输出。</p><p><img src="Pasted%20image%2020220310170808.png" /></p><h2 id="相关的工作">3. 相关的工作</h2><p>云计算环境的负载预测吸引了许多研究者的关注，同时许多学术的贡献也突出了这个问题的重要性。在这个章节，我们首先回顾了负载预测的经典的方法，然后给出了基于RNN的方法。</p><h3 id="负载预测的经典方法">负载预测的经典方法</h3><p>基于自回归（auto-regression，AR）的预测模型是利用历史的CPU时许数据去预测未来的负载。然而，这个模型本质上是严格线性的，缺乏对于在负载的云服务环境中的高可变性的负载的适应性。线性回归（LR）以及小波神经网络（WNN）被用作负载的短期预测。Kumar和Singh了通过组合人工神经网络（ANN）以及自适应的差分进化算法提出了一个负载预测模型，但是这种方法难以去决定一个合适的学习率。作者利用k临近（k-NN）提出了两个相关的过滤技术以在多种多样的多核系统中提高负载预测中的表现。但是k临近方法非常的低效，这往往导致了巨大的计算开销。Swarm以及进化优化算法被用作训练神经网络以预测主机的利用率，但是这个方法会难以选择合适的参数（例如变异系数和交叉率）。Kaur等人为特定应用的CPU利用率开发了一个集成的预测模型，这个模型会考虑8种基于回归的预测模型的平均准确率作为最终的预测结果。然而，这个方法对于需要长时间训练的模型来说非常受限。</p><p>总的来说，大多数经典的时序预测方法是基于启发式、传统的神经网络或者回归的方法。因此，它们需要负载有着明显的规律或者显著的趋势以获得预测精度。例如，不利用神经元之间的相关性，传统的神经网络不能够达成精确的有效预测。并且，这些方法主要都是在一些小批量的或者高性能的计算机系统中进行的预测，这些数据对比大批量的云数据中心往往有着更小的差异。为了有效的解决这些重要的问题并且在云服务环境中高可变的负载上达到更好的预测表现，需要更加智能的策略。RNN这种针对序列数据的框架的出现，给了云负载预测一个巨大的潜力。</p><h3 id="负载预测中基于rnn的方法">3.2 负载预测中基于RNN的方法</h3><p>RNN强调了隐藏层神经元之间的连接性以通过神经网络中历史的数据有效的处理序列问题。在过去的几年里，RNN同样被用于处理负载预测问题。Zhang等提出了一个基于RNN的模型，提高了负载预测的准确率。相似的，经典的RNN框架被用于预测云数据中心的未来的负载。结果表明RNN可以很好的处理短程依赖关系。然而，RNN耶被证明了不能有效的保证长程的预测。这是由于传统的RNN不能够解决训练过程中梯度消失的问题。因此，RNN会随着预测值以及历史信息的距离增大失去连接能力以及使用有效信息的能力。这个问题被定义为长效依赖问题。为了更好的解决长效依赖问题，LSTM作为RNN的改进版本被提出来，Song等使用了LSTM预测主机的负载，结果表明LSTM表现的比之前的RNN更优。相似的，一个使用基于LSTM的进行联结学习的模型被提出以捕捉不同的资源指标以达到对未来负载情况的精准预测。相比较于LSTM，GRU能够在更少的设置参数的情况下更容易收敛。但是，仍然有一些研究表明使用基于GRU的方法用于云负载预测存在训练效率的问题。</p><p>总的来说，大多数基于RNN的方法依赖于传统的RNN架构，这不能进不能解决梯度消失的问题，同样也不能解决长效依赖的问题。因此，他们不能够在高偏差或者显示生产环境中的云负载中学习到精准的预测结果，复杂的数据往往会让预测准确性和效率大打折扣。尽管现如今存在着部分研究提出用RNN的改进版本（例如GRU和LSTM）以解决梯度消失的问题，但云负载数据中的高维度问题仍然没有被好好考虑，这可能会导致负载预测的低准确性以及高计算复杂度。</p><p>为了解决上述的挑战，我们首先提出了TSA以在高维度的负载数据中提高压缩效率。接着，为了更好的解决负载特征间的高偏差，我们集成TSA和GRU块到RNN框架中以负载预测过程中的捕捉长效依赖。</p><h2 id="系统模型">4. 系统模型</h2><p>云服务提供商承诺提供迅速的资源分配服务以满足用户的需求，这意味着服务资源需要能够根据现有的资源利用情况作出及时的调整，这样才能使得云数据中心达到更好的负载均衡。然而，由于用户负载的变化多端，很难立即找到一个理想的资源分配策略，这样会显著的降低用户体验。同时，低效的、无理由的资源分配同样会导致不必要的分配过剩（例如，能源消耗）亦或者是对SLAs的违反。为了应对高可变和高维度的用户负载，我们提出了一个针对云数据中心的负载预测模型以最小化预测值与真实之之间的误差同时保证了预测器的执行效率。我们提出的模型的关键组成部分如图2所示。</p><p>[[Pasted image 20220311150942.png]]</p><p><em>WorkloadProcessor</em>：历史的负载数据被预测器用作学习。经过对于历史负载数据的预处理以及压缩，负载数据被作为预测器的输入。通常，历史负载数据包括系统运行状态的各种各样的指标（例如CPU利用率、内存利用率、以及磁盘I/O时间）这些往往会加重压缩过程的冗余度和复杂度。根据Google生产集群以及AmazonAWSEC2提供的数据，平均CPU利用率非常低，大约为20%和7%。数据中心的大量投入却只换来了低CPU利用率一直以来困扰着云服务提供商。因此，工业界将CPU利用率看作是一个提高云数据中心资源分配的重要指标。和很多现代在云数据中心做资源分配的工作一样，我们也将CPU利用率作为负载主要的性能指标并且在负载数据预处理阶段抽取了这个指标。我们定义<spanclass="math inline">\(\vec{X}=\left(x_{1}, x_{2}, \ldots,x_{n}\right)\)</span>，其中<span class="math inline">\(n \in\mathcal{R}\)</span>，<span class="math inline">\(x_n\)</span>是<spanclass="math inline">\(n\)</span>时刻的CPU利用率。由于不同时间区间的负载的值有着比较大的差异，原始的负载数据在进行下一步之前需要做一个归一化，这可以帮助学习算法加速手链。更重要的是，我们使用了一种机器学习中广泛的归一化的方法（也叫做规范化）：</p><p><span class="math display">\[x&#39; = \frac{x - mean(\vec{x})}{\sigma}\]</span></p><p>其中<span class="math inline">\(mean(\vec{x})\)</span>是<spanclass="math inline">\(\vec{x}\)</span>的平均值而<spanclass="math inline">\(\sigma = \sqrt{E(\vec{x}^2) -(E(\vec{x}))^2}\)</span> 即标准差</p><p>经过预处理之后，归一化的负载数据<spanclass="math inline">\(\vec{x}&#39;\)</span>被传递给负载压缩模块，高维度的冗余的负载数据会大大奖励预测的准确率以及造成高额的计算消耗。这时，一个顶部稀疏的自动编码器将被用作压缩数据，它将会有效的压缩出低维度但存在大量潜在的特征表示的负载数据。这样的负载数据会被用作一个基于门控RNN的负载预测器，TSA的详细描述将会在第5章给出。</p><p><em>Prediction Processor</em>：利用归一化并且经过压缩的历史负载数据，未来的负载情况将会被预测并且提供给云服务提供商，这些预测指标将会被用作决定合适的资源分配策略。在预测模块中，L-PAW这个基于门控RNN的学习算法，被用作提取用户负载间的长存依赖关系以在时序预测问题中提供更加准确的结果。在将L-PAW利用到负载预测之前，每个时间区间的CPU利用情况将被记录并且加入到历史负载数据中并用作RNN的输入。通过设置预测的时间长度，可以时间在不同时间维度的负载预测。L-PAW算法的细节将会在章节5给出。我们使用了均方误差（MSE）来约束负载预测的准确性：</p><p><span class="math display">\[MSE = \frac{1}{N}\sum_{i=1}^{N}(\hat(y)_i-y_i)^2\]</span></p><p>其中<span class="math inline">\(N\)</span>表示预测的时间长度，<spanclass="math inline">\(\hat{y}_i\)</span>和<spanclass="math inline">\(y_i\)</span>分别表示预测的负载情况和真实的负载情况。</p><h2 id="基于深度学习的云负载预测">5. 基于深度学习的云负载预测</h2><p>这个章节将会描述我们提出的L-PAW，一个基于深度学习算法的云负载预测器。首先，一个顶部稀疏的自动编码器（TSA）被设计用于有效的抽取具有潜在特征表达的低维度负载数据。接着，TSA和GRU块被集成进RNN以从历史数据中捕获长存依赖关系以获得精确的负载预测。</p><p>如图3所示，TSA的输入是一个负载样本的向量<spanclass="math inline">\(\vec{X} = (x_1,x_2,...,x_n)\)</span>其中<spanclass="math inline">\(n \in \mathcal{R}\)</span>而<spanclass="math inline">\(x_n\)</span>表示n时刻的CPU利用率。与SA相似的是，TSA同样要拟合一个函数<spanclass="math inline">\(y_n = f(Wx_n+b) \approx x_n\)</span>，让输出<spanclass="math inline">\(y_n\)</span>能够尽量的逼近输入<spanclass="math inline">\(x_n\)</span>。特别的是，SA用的是线性激活函数的组合和固定的权重，着往往由于对于隐藏单元的过度使用从而导致了学习的效率非常低下。我们提出的TSA可以被视作一个SA的升级版本，顶部的有着最高激活度的k个隐藏单元被玄宗来重构输入数据而不是像SA一样使用所有的隐藏单元。在前馈的过程中，每一个隐藏单元平均激活值<spanclass="math inline">\(\hat{\rho}\)</span>按照如下的方式计算：</p><p><span class="math display">\[\hat{\rho} = \frac{1}{N}\sum_{i=1}^{n}[a^{(h)}(x_i)]\]</span></p><p>其中<spanclass="math inline">\(a^{(h)}\)</span>是隐藏层的激活函数</p><p><img src="Pasted%20image%2020220311160615.png" /></p><p>接着，所有的隐藏单元会按照他们的<spanclass="math inline">\(\hat{\rho}\)</span>值进行排序，前k个隐藏单元可以被找出来，用一个向量表示<spanclass="math inline">\(\tau ={top}_k(\hat{\rho})\)</span>。因此，非线性的计算仅仅只会在传递<spanclass="math inline">\({top}_k(\hat{\rho})\)</span>的时候发生，这相比于SA极大的减少了计算复杂度。更加重要的是，<spanclass="math inline">\(k\)</span>值影响着压缩前后负载数据的相似程度。。例如，当使用一个较小的<spanclass="math inline">\(k\)</span>时TSA不能够完全捕捉到原始数据的特征（隐藏层太少了），这会使得压缩数据严重变形，反之，使用一个较大的<spanclass="math inline">\(k\)</span>值，TSA将会携带大量的冗余信息（隐藏层过多），这又会加剧接下来预测工作的计算复杂度。我们提出的TSA算法的关键步骤由算法1描述。算法1的复杂度是<spanclass="math inline">\(O(n)\)</span>的，与隐藏层的大小<spanclass="math inline">\(n\)</span>线性相关。</p><p><img src="Pasted%20image%2020220311160628.png" /></p><p>因此，复杂数据的压缩问题转换成了一个通过最小化损失函数<spanclass="math inline">\(J_{TSA}(W, b)\)</span>计算权重<spanclass="math inline">\(W\)</span>以及偏差<spanclass="math inline">\(b\)</span>的问题。特别的，标准神经网络的损失函数被定义为：</p><p><span class="math display">\[J(W, b) =\frac{\lambda}{2n}\sum_{l=1}^{2}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(W^{(l)}_{ji})^2+\frac{1}{n}\sum_{i=1}^{n}(\frac{1}{2}||x_i-y_i||^2)\]</span></p><p>上式第一项是为了避免过拟合的正则项，第二项是原始负载数据<spanclass="math inline">\(x_i\)</span>和经过编码后的<spanclass="math inline">\(y_i\)</span>之间的均方误差。</p><p>为了在求导计算中引入KL散度，隐藏层在反向传播的求导被修改成了：</p><p><span class="math display">\[\delta^{(h)}_i =[\sum_{j=1}^{N_o}\delta^{(h)}_jW^{(h)}_{ji}+\beta(-\frac{\rho}{\hat{\rho}_i}+\frac{1-\rho}{1-\hat{\rho}_i})]f&#39;(z^{(h)}_i)\]</span></p><p>其中<spanclass="math inline">\(N_o\)</span>是输出神经元的格式，而<spanclass="math inline">\(f&#39;(z^{(h)}_i)\)</span>是激活函数<spanclass="math inline">\(f(z^{(h)}_i)=a^{(h)}_i\)</span>的导数</p><p>接着，我们将被压缩过的负载数据集作为原始数据集的高层表示，并且将它们用作基于RNN的负载预测模型的输入<spanclass="math inline">\(\vec{X^c}=(X^c_1,x^c_2,...,x^c_t)\)</span>，假设模型的输出是<spanclass="math inline">\(\vec{Y}=(\hat{y}_1,\hat{y}_2,...,\hat{y}_t)\)</span>，那么模型将会通过比较预测的负载数据<spanclass="math inline">\(\hat{y}_t\)</span>和<spanclass="math inline">\(t+1\)</span>时刻真实的负载数据<spanclass="math inline">\(x^c_{t+1}\)</span>之间的误差。特别的，通过时间的反向传播（BPTT）被用作RNN的训练算法，在隶书的负载数据和预测的负载数据之间仅存在一个短的时间区间。RNN可以学习到有用的信息来做预测。然而，RNN要读入和更新所有先前的信息，随着时间增加在RNN中累计的梯度就会非常接近于0，导致RNN的网络参数不能被有效的更新，最后学习失败。这个问题就是梯度消失问题，同样也可以被解释为对于长存依赖学习能力的匮乏。因此，长时间以前的历史负载数据在传统的RNN结构中不能被有效的利用。</p><p>为此，我们提出来L-PAW以更好的解决以上的文同。基于TSA抽取出来的负载的潜在特征表达，我们将经典RNN结构中的隐藏层替换成了GRU块。算法2描述了L-PAW的关键步骤。在调用TSA以获得压缩的负载数据之后，我们设置了一个衰减参数<spanclass="math inline">\(\lambda\)</span>以成段的控制学习率<spanclass="math inline">\(\gamma\)</span>，旨在不同的训练阶段达成更有效的学习。</p><p><img src="Pasted%20image%2020220314210819.png" /></p><p>为了解决传统RNN中出现的梯度消失的问题，一些门结构的RNN被发明，例如LSTM和GRU。相比于LSTM，GRU有更少的参数，可以达到更高的学习效率。于传统RNN不同的是，GRU利用门结构以有选择性的读入和更新以前的信息。因此，GRU只保留对于预测有用的信息而过滤掉不相关的信息。同时，GRU通过门结构以及直接传递储存的历史信息自动的在不同的网络层中建立短链接。因此，GRU可以通过设置不同的门结构，重新组织传统RNN的参数来解决梯度消失的问题。GRU的核心思想是使隐藏单元保存一些长期的记忆，这可以使得梯度经过很多步的处理。GRU是从LSTM简化而来的，它将LSTM的遗忘门和输入门合并成了一个更新门。因此，GRU有两种类型的门，一种是更新门<spanclass="math inline">\(z_t\)</span>另一种是重置门<spanclass="math inline">\(r_t\)</span>。如图4所示，我们描绘了L-PAW算法中GRU块的结构。类似于LSTM，两个门的更新是基于现在的输入<spanclass="math inline">\(x^c_t\)</span>以及以往的隐藏状态<spanclass="math inline">\(\hat{y}_{t-1}\)</span>。新的记忆数据<spanclass="math inline">\(\tilde{y_t}\)</span>被视作是现在时刻<spanclass="math inline">\(t\)</span>的新数据，遗忘门<spanclass="math inline">\(r_t\)</span>被用作控制以前的记忆数据是否需要被丢弃。除此之外，更新门<spanclass="math inline">\(Z_t\)</span>被用作控制先前的记忆内容<spanclass="math inline">\(\hat{y}_{t-1}\)</span>以及新的记忆内容<spanclass="math inline">\(\tilde{y_t}\)</span>是应该被添加进来还是应该被遗忘。因此，GRU块的输出<spanclass="math inline">\(\hat{y}_t\)</span>（即预测的负载数据）可以基于更新门<spanclass="math inline">\(Z_t\)</span>计算。算法儿的复杂度与模型的容量（即模型的参数）正相关，可以被表示为<spanclass="math inline">\(O(3(n^2+nm+n))\)</span>，其中<spanclass="math inline">\(m\)</span>是输入的尺寸，<spanclass="math inline">\(n\)</span>是隐藏层的尺寸，并且存在3个操作集合需要权重指标（其中两个是更新门和重置门，另外一个是新的记忆数据）特别的，GRU是通过小批次随机梯度下降（SGD）训练的，以获得更高的准确性。</p><p><img src="Pasted%20image%2020220314211717.png" /></p><p>TSA和GRU块的结合使得传统的RNN能够从历史负载数据中更有效的学习到长存记忆以来。无论历史记忆是否关键，更新门会为了在经过很多步之后保存潜在的负载特征而关闭。另外，重置门使得GRU块可以通过设置何时保存的记忆不在必要以合理的利用模型的容量。因此，提出的L-PAW是基于一个简化版的有有着更少门结构的LSTM得到的，同时可以通过利用高层的负载数据的特征表示相比于GRU达到更快的收敛速度。相对的，LSTM有着更多的门以及参数，它需要大量的训练样本以及更长的训练时间以训练一个优秀的模型。而GRU可能会因为经典SA对于隐藏层的过度使用而丢失训练效率。</p><h2 id="实验">6. 实验</h2><p>在本章节，我们首先给出了模拟环境的设置以及我们实验的数据集，接着，我们验证了提出的T-PWA算法的表现以及比较了基于RNN的方法以及其他在云数据中心中做负载预测的经典算法。</p><h3 id="模拟环境设置及数据集">6.1 模拟环境设置及数据集</h3><p>我们基于TensorFlow1.4.0部署了云负载预测模型。实验中我们使用了真实生产环境下的数据集。其中一个是谷歌集群使用数据，它包含了超过125000个在谷歌云数据中心运行的机器从2011年5月以来的数据。第二个是阿里巴巴的集群数据，它包含了4000个有着运行时资源使用率机器的数据。第三个是由Dinda收集的的基于DUX的集群数据。在我们的誓言中，我们将CPU使用率视作主要的性能指标。更重要的是，我们从谷歌数据集中随机选择了1000个超过29天的机器，每一个机器包括接近100000个数据。相似的，我们也从阿里巴巴数据集中随机选择了1000个超过8天的机器，每个机器包括接近7000条数据。接着，我们抽取了若干条与负载预测相关的潜在的指标，包括机器编号、启动时间、停止时间、CPU利用率、内存利用率以及磁盘I/O。如图5和图6所示，我们展示了谷歌以及阿里巴巴数据集中一个机器每一天以及每一分钟的负载波动。至于基于DUX的集群数据已经根据负载的特征进行了分类，我们从两个特定的机器中选择了两个数据集，一个包含15天以来1296000高度自相关的负载数据，另一个包含1123200条高度周期性的13天的数据。图7和图8展示了这两种基于DUX集群的机器的每一天和每一分钟的负载波动。我们可以从图5，6，7，8中发现，谷歌和阿里巴巴的负载存在着更加随机的特点，而基于DUX集群的负载则表现出高自相关和周期性。平均来说，在预处理之后，每一个host机器的负载样本大小约为8000。我们按照批次将数据喂入我们的预测模型中。更细节的是，我们随机将数据集划分为了三个部分，训练集（50%）、验证集（25%）以及测试集（25%）。训练集用作模型的训练（计算神经网络的权重），验证集用于模型的选择（选择超参数以及避免过拟合），而测试机用于验证选择的最优模型的表现。另外，训练的总轮数是100轮，初始的学习率是0.03，被去除的反向传播步数是32，一个批次的大小是128。</p><p><img src="Pasted%20image%2020220314215742.png" /></p><p><img src="Pasted%20image%2020220314215834.png" /></p><p><img src="Pasted%20image%2020220314215853.png" /></p><p><img src="Pasted%20image%2020220314215822.png" /></p><h3 id="实验结果">6.2 实验结果</h3><p>我们首先验证了提出的用于压缩负载数据的TSA算法的损失函数的值来表现压缩效果，在谷歌数据集上用不同数量的top隐藏单元，k值从32变化到512，图9a描述了损失函数的值会在大约50个训练轮次后显著下降并且最终以不同数量的top隐藏单元数量收敛。更特别的是，当top隐藏单元的数量小的时候（例如<spanclass="math inline">\(k\le64)\)</span>损失函数的值相当高。因为TSA的神经网络必须大面积的重构以小的top隐藏单元数适应压缩需求。当隐藏单元数较大时（例如<spanclass="math inline">\(k\ge128)\)</span>，不同隐藏单元数目的损失函数的值没有很显著的差异，这是因为神经网络结构已经具备了从原始数据中学习潜在特征表示的能力。因此，我们设置隐藏神经单元的数目为128（即<spanclass="math inline">\(k=128\)</span>）并且在接下来的实验中一直保持这个设定。同时，我们也画出了负载数据在使用TSA以不同数目的隐藏单元压缩的前后对比。如图9b，9c，9e以及9f所示，负载压缩的TSA算法在恰当设定top隐藏单元数目时时非常有效的，它能够提供有效的特征表示，极大的减少了我们提出的负载预测算法的计算复杂度。</p><p><img src="Pasted%20image%2020220314231238.png" /></p><p>基于谷歌数据集和经过TSA压缩预处理过后的负载数据集，我们验证了提出的L-PAW算法以及其他最近提出的用于负载预测的基于RNN的方法，包括RNN、LSTM、GRU以及ESN。我们同时比较了这些方法的预测准确率和学习效率。图10展现了不同基于RNN方法在不同预测长度层级上的均方误差。总的来说，这些方法的均方误差均会随着预测长度提升而变大。更重要的是，在第二层级的预测中，L-PAW和其他的方法在预测准确率方面没有显著的差异。当预测长度提升的时候（从分钟级预测到天级的预测），L-PAW不仅在预测准确率上领先于其他基于RNN的算法，同时也表现出了更大的差距在性能提升上。这是因为L-PAW可以解决梯度消失的问题并且捕获长存记忆依赖关系。结果表明L-PAW对于高维度、高可变性的云负载数据的预测相比于其他基于RNN的方法是更有效的。</p><p><img src="Pasted%20image%2020220315132839.png" /></p><p>接着，我们比较了提出的L-PAW算法和其他基于RNN的方法在不同预测长度下的学习效率。如图11所示，以上方法在不同预测长度下的平均预测时间被记录下来。RNN因为其简单的神经网络结构在不同层级的预测长度下表现出最短的训练时间。然而，RNN的训练准确率要比其他基于RNN的方法更差，这个结果如图10所示。而基于门控的RNN方法，例如GRU则因为其更少的门配置相比于LSTM有着更短的训练时间。然而，GRU的平均训练时间比ESN要高得多，因为ESN通过自编码器完成负载压缩减少了计算复杂度，相对的，L-PAW通过集成TSA和GRU快到RNN中相比于ESN达到了更少的训练时间。因此，L-PAW可以在随着预测长度上升时预测准确率和学习效率之间相比于其他基于RNN的方法达到更好的均衡。</p><p><img src="Pasted%20image%2020220315133438.png" /></p><p>接着，我们验证了L-PAW在不同类型的负载数据在不同级别的预测长度下的表现，包括高度随机、高度自相关一集高度周期性的负载。如表1所示，随着预测长度的增加，L-PAW在高自相关和高周期性的数据在CPU使用率层面可以达到并保持高的预测准确性。在面对高随机的负载数据是，L-PAW仍然可以保持好的预测结果尽管负载的变化为负载预测带来了极大的困难。例如，图12描述了L-PAW在不同类型的负载数据在秒级的预测，展示了L-PAW在CPU、内存以及磁盘I/O层面可以达到高准确率的负载预测。如图13和图14所示，L-PAW在应对谷歌（内存使用）和阿里巴巴（磁盘I/O利用）数据中心高随机性的数据仍然可以表现出高预测准确性。甚至对于天级别的预测，L-PAW可以准确的预测出未来负载变化的趋势。因此，以上的结果表明了L-PAW在不同层级的预测长度下面对不同类型的负载数据具有非常强大的预测能力。</p><p><img src="Pasted%20image%2020220315134245.png" /></p><p>最终，我们将L-PAW和其他负载预测的经典方法进行了比较，包括自动回归、线性回归以及人工神经网络。在表现预测准确性的MSE方面，图15描绘了不同级别的预测长度下的不同负载预测方法的在谷歌数据集MSE的累计分布函数（CDF），如图15a所示，对于秒级别的预测，L-PAW能够达到的秒级别的值在CDF趋近于1的时候相比于其他所有方法都要更低。如图15b及图15d所示，随着预测长度的增加，L-PAW相比于其他经典方法达到了更加明显的性能提升。这是因为这些经典的方法不能够有效的实现长效的预测或者针对高度随机负载数据的预测。相反的，L-PAW可以更好的解决这些问题，因为它可以通过TSA从原始数据中抽取特征表达并且通过引入GRU块捕捉长效依赖。</p><p><img src="Pasted%20image%2020220315134853.png" /></p><h2 id="结论">7. 结论</h2><p>适应性的和有效的负载预测对于有效的资源分配来说是必要的。然而，负载预测并没有很好的解决高变化和高维度的负载数据带来的挑战。在这篇文章中，我们首先设计了一个top稀疏自编码器（TSA）来有效的从原始的高维数据中抽取必要特征表达。接着，集成了TSA和GRU块的L-PAW算法被提出以达到适应性强、准确性高的负载预测。我们进行的实验使用了谷歌公司和阿里巴巴公司云数据中心以及基于DUX的集群的真实生产环境下的负载数据集，表明在高自相关的、高周期性的以及高度随机的数据中，L-PAW可以达到MSE约束的高预测准确性。并且，随着预测长度的增加，预测误差仅仅只有很小的增大，这同样也表明了L-PAW的强大能力。同时，L-PAW也比经典的RNN、LSTM、GRU以及ESN在负载预测上对于MSE的提升表现的要好，同时还达到了非常高的学习效率。另外，随着预测长度的提升，L-PAW相对于其他方法达到了更大的预测准确性的提升，这意味着L-PAW有着在云负载预测问题中解决长效依赖问题的能力。机遇准确的、高效的云负载预测，我们未来的工作是要为资源分配去探索一个适应性强的、机遇深度强化学习的策略，以应对负载的、高动态的云计算环境。</p>]]></content:encoded>
      
      
      <category domain="https://jiengup.github.io/categories/Research/">Research</category>
      
      
      <category domain="https://jiengup.github.io/tags/workload/">workload</category>
      
      <category domain="https://jiengup.github.io/tags/prediction/">prediction</category>
      
      <category domain="https://jiengup.github.io/tags/deep/">deep</category>
      
      <category domain="https://jiengup.github.io/tags/learning/">learning</category>
      
      
      <comments>https://jiengup.github.io/2022/10/25/11/18/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>*unix下使用gdb调试代码</title>
      <link>https://jiengup.github.io/2022/08/15/16/07/</link>
      <guid>https://jiengup.github.io/2022/08/15/16/07/</guid>
      <pubDate>Mon, 15 Aug 2022 08:07:01 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;unix下使用gdb调试代码&quot;&gt;*unix下使用gdb调试代码&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;这篇博文实际上是&lt;a href=&quot;http://www.wustl.edu/&quot;&gt;Washington
University&lt;/a&gt;课程&lt;a
href=&quot;htt</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="unix下使用gdb调试代码">*unix下使用gdb调试代码</h1><blockquote><p>这篇博文实际上是<a href="http://www.wustl.edu/">WashingtonUniversity</a>课程<ahref="http://www.cs.wustl.edu/~cdgill/courses/cs342">CS342</a>的中文翻译</p></blockquote><h2 id="源代码">源代码</h2><p>下面的例子所使用的源代码是</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main.cc</span></span><br><span class="line"><span class="comment">// Andrew Gilpin</span></span><br><span class="line"><span class="comment">// agg1@cec.wustl.edu</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// This file contains the example program used in the gdb debugging</span></span><br><span class="line"><span class="comment">// tutorial. The tutorial can be found on the web at</span></span><br><span class="line"><span class="comment">// http://students.cec.wustl.edu/~agg1/tutorial/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> number_instantiated = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Node</span> (<span class="type">const</span> T &amp;value, Node&lt;T&gt; *next = <span class="number">0</span>) : <span class="built_in">value_</span>(value), <span class="built_in">next_</span>(next) &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Creating Node, &quot;</span></span><br><span class="line">         &lt;&lt; ++number_instantiated</span><br><span class="line">         &lt;&lt; <span class="string">&quot; are in existence right now&quot;</span> &lt;&lt; endl;</span><br><span class="line">  &#125;</span><br><span class="line">  ~<span class="built_in">Node</span> () &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Destroying Node, &quot;</span></span><br><span class="line">         &lt;&lt; --number_instantiated</span><br><span class="line">         &lt;&lt; <span class="string">&quot; are in existence right now&quot;</span> &lt;&lt; endl;</span><br><span class="line">    next_ = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">Node&lt;T&gt;* <span class="title">next</span> <span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> next_; &#125;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">next</span> <span class="params">(Node&lt;T&gt; *new_next)</span> </span>&#123; next_ = new_next; &#125;;</span><br><span class="line">  <span class="function"><span class="type">const</span> T&amp; <span class="title">value</span> <span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> value_; &#125;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">value</span> <span class="params">(<span class="type">const</span> T &amp;value)</span> </span>&#123; value_ = value; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="built_in">Node</span> ();</span><br><span class="line">  T value_;</span><br><span class="line">  Node&lt;T&gt; *next_;</span><br><span class="line">&#125;;</span><br><span class="line">  </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinkedList</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">LinkedList</span> () : <span class="built_in">head_</span>(<span class="number">0</span>) &#123;&#125;;</span><br><span class="line">  ~<span class="built_in">LinkedList</span> () &#123; <span class="built_in">delete_nodes</span> (); &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// returns 0 on success, -1 on failure</span></span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">insert</span> <span class="params">(<span class="type">const</span> T &amp;new_item)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ((head_ = <span class="keyword">new</span> <span class="built_in">Node</span>&lt;T&gt;(new_item, head_)) != <span class="number">0</span>) ? <span class="number">0</span> : <span class="number">-1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// returns 0 on success, -1 on failure</span></span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">remove</span> <span class="params">(<span class="type">const</span> T &amp;item_to_remove)</span> </span>&#123;</span><br><span class="line">    Node&lt;T&gt; *marker = head_;</span><br><span class="line">    Node&lt;T&gt; *temp = <span class="number">0</span>;  <span class="comment">// temp points to one behind as we iterate</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (marker != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (marker-&gt;<span class="built_in">value</span>() == item_to_remove) &#123;</span><br><span class="line">        <span class="keyword">if</span> (temp == <span class="number">0</span>) &#123; <span class="comment">// marker is the first element in the list</span></span><br><span class="line">          <span class="keyword">if</span> (marker-&gt;<span class="built_in">next</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">            head_ = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">delete</span> marker; <span class="comment">// marker is the only element in the list</span></span><br><span class="line">            marker = <span class="number">0</span>;</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            head_ = <span class="keyword">new</span> <span class="built_in">Node</span>&lt;T&gt;(marker-&gt;<span class="built_in">value</span>(), marker-&gt;<span class="built_in">next</span>());</span><br><span class="line">            <span class="keyword">delete</span> marker;</span><br><span class="line">            marker = <span class="number">0</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          temp-&gt;<span class="built_in">next</span> (marker-&gt;<span class="built_in">next</span>());</span><br><span class="line">          <span class="keyword">delete</span> temp;</span><br><span class="line">          temp = <span class="number">0</span>;</span><br><span class="line">          <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      marker = <span class="number">0</span>;  <span class="comment">// reset the marker</span></span><br><span class="line">      temp = marker;</span><br><span class="line">      marker = marker-&gt;<span class="built_in">next</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;<span class="comment">// failure</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">print</span> <span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    Node&lt;T&gt; *marker = head_;</span><br><span class="line">    <span class="keyword">while</span> (marker != <span class="number">0</span>) &#123;</span><br><span class="line">      cout &lt;&lt; marker-&gt;<span class="built_in">value</span>() &lt;&lt; endl;</span><br><span class="line">      marker = marker-&gt;<span class="built_in">next</span>();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">delete_nodes</span> <span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    Node&lt;T&gt; *marker = head_;</span><br><span class="line">    <span class="keyword">while</span> (marker != <span class="number">0</span>) &#123;</span><br><span class="line">      Node&lt;T&gt; *temp = marker;</span><br><span class="line">      <span class="keyword">delete</span> marker;</span><br><span class="line">      marker = temp-&gt;<span class="built_in">next</span>();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">        </span><br><span class="line">  Node&lt;T&gt; *head_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span> <span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  LinkedList&lt;<span class="type">int</span>&gt; *list = <span class="keyword">new</span> <span class="built_in">LinkedList</span>&lt;<span class="type">int</span>&gt; ();</span><br><span class="line"></span><br><span class="line">  list-&gt;<span class="built_in">insert</span> (<span class="number">1</span>);</span><br><span class="line">  list-&gt;<span class="built_in">insert</span> (<span class="number">2</span>);</span><br><span class="line">  list-&gt;<span class="built_in">insert</span> (<span class="number">3</span>);</span><br><span class="line">  list-&gt;<span class="built_in">insert</span> (<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">  cout &lt;&lt; <span class="string">&quot;The fully created list is:&quot;</span> &lt;&lt; endl;</span><br><span class="line">  list-&gt;<span class="built_in">print</span> ();</span><br><span class="line"></span><br><span class="line">  cout &lt;&lt; endl &lt;&lt; <span class="string">&quot;Now removing elements:&quot;</span> &lt;&lt; endl;</span><br><span class="line">  list-&gt;<span class="built_in">remove</span> (<span class="number">4</span>);</span><br><span class="line">  list-&gt;<span class="built_in">print</span> ();</span><br><span class="line">  cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">  list-&gt;<span class="built_in">remove</span> (<span class="number">1</span>);</span><br><span class="line">  list-&gt;<span class="built_in">print</span> ();</span><br><span class="line">  cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">  list-&gt;<span class="built_in">remove</span> (<span class="number">2</span>);</span><br><span class="line">  list-&gt;<span class="built_in">print</span> ();</span><br><span class="line">  cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">  list-&gt;<span class="built_in">remove</span> (<span class="number">3</span>);</span><br><span class="line">  list-&gt;<span class="built_in">print</span> ();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">delete</span> list;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外还配有一个简单的Makefile</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CXX = g++</span><br><span class="line">FLAGS = -ggdb -Wall</span><br><span class="line"></span><br><span class="line"><span class="section">main: main.cc</span></span><br><span class="line">$&#123;CXX&#125; $&#123;FLAGS&#125; -o main main.cc</span><br><span class="line"></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">rm -f main</span><br></pre></td></tr></table></figure><p>这份代码中定义了两个类，一个节点类以及一个链表类。同时实现了一个简单的测试。</p><h2 id="准备">准备</h2><h3 id="调试记号">调试记号</h3><p><code>gdb</code>可以使用<code>g++</code>生成的调试记号</p><p>当<code>gdb</code>调试有与其对应的记号的程序时是最高效的，通过给编译命令加上<code>-g</code>参数可以实现给可执行程序绑定记号</p><h2 id="调试">调试</h2><h3 id="什么时候需要调试工具">什么时候需要调试工具</h3><p>首先需要知道的是，调试工具是无可避免的。每一个程序员都无可厚非的曾经在职业生涯的某一刻调试过一段代码。调试有非常多的方式，从在屏幕上打印一些消息，到使用专业的调试工具，或者仅仅是通过思考程序的行为然后做一些猜想来使得程序正确。</p><p>在一个bug被修复之前，这个bug的源头需要被正确的定位。例如，<strong>段错误</strong>发生时，我们需要知道哪一行代码引发了段的错误。当一行有问题的代码被找到的时候，我们还需要知道在这个函数中变量的值是什么，又是谁调用了这个函数，以及为什么这个错误会发生。使用调试工具可以很简单的解答上述的问题。</p><p>编译运行<ahref="#源代码">源代码</a>中的程序，它会打印一些信息，然后它会表示收到了一个段错误，接着程序就崩溃了。我们就要开始调试这个程序。</p><h3 id="载入一个程序">载入一个程序</h3><p>通过编译运行后，现在有了一个可执行文件（下面用<code>main</code>来表示）并且你需要去调试它。首先必须启动调试工具<code>gdb</code>，同时可以告诉它你需要调试哪个文件，比如输入<code>gdb main</code>执行的时候，它就会是下面这个样子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@a852a26669ff:/bustub# gdb main</span><br><span class="line">GNU gdb (Ubuntu 8.1.1-0ubuntu1) 8.1.1</span><br><span class="line">Copyright (C) 2018 Free Software Foundation, Inc.</span><br><span class="line">License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;</span><br><span class="line">This is free software: you are free to change and redistribute it.</span><br><span class="line">There is NO WARRANTY, to the extent permitted by law.  Type <span class="string">&quot;show copying&quot;</span></span><br><span class="line">and <span class="string">&quot;show warranty&quot;</span> <span class="keyword">for</span> details.</span><br><span class="line">This GDB was configured as <span class="string">&quot;aarch64-linux-gnu&quot;</span>.</span><br><span class="line">Type <span class="string">&quot;show configuration&quot;</span> <span class="keyword">for</span> configuration details.</span><br><span class="line">For bug reporting instructions, please see:</span><br><span class="line">&lt;http://www.gnu.org/software/gdb/bugs/&gt;.</span><br><span class="line">Find the GDB manual and other documentation resources online at:</span><br><span class="line">&lt;http://www.gnu.org/software/gdb/documentation/&gt;.</span><br><span class="line">For <span class="built_in">help</span>, <span class="built_in">type</span> <span class="string">&quot;help&quot;</span>.</span><br><span class="line">Type <span class="string">&quot;apropos word&quot;</span> to search <span class="keyword">for</span> commands related to <span class="string">&quot;word&quot;</span>...</span><br><span class="line">Reading symbols from main...done.</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p><code>gdb</code>现在在等待用户的进一步输入，我们需要运行这个程序因此调试工具可以帮助我们了解程序崩溃的时候发生了什么，在<code>gdb</code>交互程序中输入<code>run</code>，会有如下的消息打出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">(gdb) run</span><br><span class="line">Starting program: /bustub/main</span><br><span class="line">warning: Error disabling address space randomization: Operation not permitted</span><br><span class="line">Creating Node, 1 are in existence right now</span><br><span class="line">Creating Node, 2 are in existence right now</span><br><span class="line">Creating Node, 3 are in existence right now</span><br><span class="line">Creating Node, 4 are in existence right now</span><br><span class="line">The fully created list is:</span><br><span class="line">4</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">Now removing elements:</span><br><span class="line">Creating Node, 5 are in existence right now</span><br><span class="line">Destroying Node, 4 are in existence right now</span><br><span class="line">4</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Program received signal SIGSEGV, Segmentation fault.</span><br><span class="line">0x0000aaaac4c31304 in Node&lt;int&gt;::next (this=0x0) at main.cc:29</span><br><span class="line">29  Node&lt;T&gt;* next () const &#123; return next_; &#125;</span><br></pre></td></tr></table></figure><p>程序崩溃了，我们来看看发生了什么</p><h3 id="检查崩溃">检查崩溃</h3><p>我们现在已经可以知道程序在<em>main.cc</em>的28行发生了崩溃，<code>this</code>指针指向了0，并且我们可以知道这一行执行的代码是什么。但是我们同样也想知道是谁条约9那个了这个额函数，并且我们希望能够测试函数中的变量的值。通过<code>gdb</code>交互，输入<code>backtrace</code>可以得到以下的输出:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(gdb) backtrace</span><br><span class="line">#0  Node&lt;int&gt;::next (this=0x0) at main.cc:28</span><br><span class="line">#1  0x2a16c in LinkedList&lt;int&gt;::remove (this=0x40160, </span><br><span class="line">    item_to_remove=@0xffbef014) at main.cc:77</span><br><span class="line">#2  0x1ad10 in main (argc=1, argv=0xffbef0a4) at main.cc:111</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>我们知道了调用的函数以及局部变量，同时也知道了是哪个函数调用了这个函数并且它的调用参数是什么。例如，我们知道了是通过<code>LinkedList&lt;int&gt;::remove()</code>执行的调用，并且参数<code>item_to_remove</code>在<em>0xffbef014</em>这个地址。通过<code>item_to_remove</code>的值可以帮助我们理解我们的bug，因此我们想要看看地址为<em>0xffbef014</em>的值。这个可以通过使用<code>x</code>命令完成，当运行这个命令时命令行给出了下面的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(gdb) x 0xffbef014</span><br><span class="line">0xffbef014:0x00000001</span><br></pre></td></tr></table></figure><p>因此当使用参数<em>1</em>运行<code>LinkedList&lt;int&gt;::remove</code>时程序会发生崩溃。我们现在将问题集中到了解决一个具体的参数值上。</p><h3 id="条件断点">条件断点</h3><p>现在我们知道了何时在何处发生了段错误，我们想要观察程序在发生错误之前是否正确的执行。可行的一个方法是一步一步执行，直到我们达到了想要的地方。</p><p>如果你曾经使用过调试工具那么一定对断点不陌生。最基础的，断点是源代码中的一行，调试工具会在这行停止运行。在我们的例子中，我们想要去观察<code>LinkedList&lt;int&gt;::remove()</code>中的代码，所以我们可以设置一个在52行的断点。有的时候你可能不知道确切的行号，所以你也可以告诉调试工具你需要停在哪个函数中，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(gdb) break LinkedList&lt;int&gt;::remove</span><br><span class="line">Breakpoint 1 at 0x29fa0: file main.cc, line 52.</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>因此现在<em>main.cc</em>中的52行设置好了断点1（断点都有一个编号的原因在于我们稍后可以通过这个编号指向断点，例如想要删除它的时候）。当程序执行时，每当跑到52行的时候它会将控制权交还给调试工具。当这个方法会被调用很多次的时候断点可能就没有那么有效了，因此条件断电可以帮助我们解决问题。例如，我们知道程序会在<code>LinkedList&lt;int&gt;::remove()</code>以参数1调用的时候发狠恶搞崩溃，因此我们可能想告诉调试器仅仅在<em>item_to_remove</em>为1的时候才在52行停下来，这可以通过以下的命令实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(gdb) condition 1 item_to_remove==1</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><h3 id="步进">步进</h3><p>继续上述的例子，我们设置了一个条件断点，现在希望可以一次一步的执行这个函数尝试定位错误的源头，这个可以通过使用<code>step</code>命令实现。<code>gdb</code>有一个很好的特性，当不输入命令按下回车的时候，上一次执行的命令竟会被重复执行一次。我们可以通过使用这个特性不断的执行<code>step</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">(gdb) run</span><br><span class="line">The program being debugged has been started already.</span><br><span class="line">Start it from the beginning? (y or n) y</span><br><span class="line"></span><br><span class="line">Starting program: /home/cec/s/a/agg1/.www-docs/tutorial/main </span><br><span class="line">Creating Node, 1 are in existence right now</span><br><span class="line">Creating Node, 2 are in existence right now</span><br><span class="line">Creating Node, 3 are in existence right now</span><br><span class="line">Creating Node, 4 are in existence right now</span><br><span class="line">The fully created list is:</span><br><span class="line">4</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">Now removing elements:</span><br><span class="line">Creating Node, 5 are in existence right now</span><br><span class="line">Destroying Node, 4 are in existence right now</span><br><span class="line">4</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Breakpoint 1, LinkedList&lt;int&gt;::remove (this=0x40160, </span><br><span class="line">    item_to_remove=@0xffbef014) at main.cc:52</span><br><span class="line">52    Node&lt;T&gt; *marker = head_;</span><br><span class="line">(gdb) step</span><br><span class="line">53    Node&lt;T&gt; *temp = 0;  // temp points to one behind as we iterate</span><br><span class="line">(gdb) </span><br><span class="line">55    while (marker != 0) &#123;</span><br><span class="line">(gdb) </span><br><span class="line">56      if (marker-&gt;value() == item_to_remove) &#123;</span><br><span class="line">(gdb) </span><br><span class="line">Node&lt;int&gt;::value (this=0x401b0) at main.cc:30</span><br><span class="line">30  const T&amp; value () const &#123; return value_; &#125;</span><br><span class="line">(gdb) </span><br><span class="line">LinkedList&lt;int&gt;::remove (this=0x40160, item_to_remove=@0xffbef014)</span><br><span class="line">    at main.cc:75</span><br><span class="line">75      marker = 0;  // reset the marker</span><br><span class="line">(gdb) </span><br><span class="line">76      temp = marker;</span><br><span class="line">(gdb) </span><br><span class="line">77      marker = marker-&gt;next();</span><br><span class="line">(gdb) </span><br><span class="line">Node&lt;int&gt;::next (this=0x0) at main.cc:28</span><br><span class="line">28  Node&lt;T&gt;* next () const &#123; return next_; &#125;</span><br><span class="line">(gdb) </span><br><span class="line"></span><br><span class="line">Program received signal SIGSEGV, Segmentation fault.</span><br><span class="line">Node&lt;int&gt;::next (this=0x0) at main.cc:28</span><br><span class="line">28  Node&lt;T&gt;* next () const &#123; return next_; &#125;</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure><p>在输入<code>run</code>之后，<code>gdb</code>会询问我们是否想要重新运行程序，我们重新运行后，程序会在既定的地方停下来，接着我们输入<code>step</code>然后回车以步进程序，注意调试工具会进入调用的函数，如果你不希望这样，你可以使用<code>next</code></p><p>程序中的错误已经非常明显了，75行<em>marker</em>被设置成0，但是77行<em>maker</em>的成员被访问，因为程序不能够访问内存地址为0的对象的成员，段错误就发生了，在这个例子中，简单的删除75行即可避免错误。</p><h2 id="更多信息">更多信息</h2><p>这个文档仅仅包含了最简单和必要的命令以开始使用<code>gdb</code>，有关于<code>gdb</code>的更多信息可以查看<code>man gdb</code>的手册页面或者浏览<ahref="https://sourceware.org/gdb/current/onlinedocs/gdb/">一个非常长的说明</a>。也可以通过在运行<code>gdb</code>的时候输入<code>help</code>来获取在线的帮助。</p>]]></content:encoded>
      
      
      <category domain="https://jiengup.github.io/categories/Tools/">Tools</category>
      
      
      <category domain="https://jiengup.github.io/tags/tools/">tools</category>
      
      <category domain="https://jiengup.github.io/tags/debugging/">debugging</category>
      
      
      <comments>https://jiengup.github.io/2022/08/15/16/07/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>从零搭建zotero到obsidian的工作流</title>
      <link>https://jiengup.github.io/2022/08/10/19/55/</link>
      <guid>https://jiengup.github.io/2022/08/10/19/55/</guid>
      <pubDate>Wed, 10 Aug 2022 11:55:04 GMT</pubDate>
      
        
        
      <description>&lt;h1
id=&quot;从零搭建zotero到obsidian的工作流&quot;&gt;从零搭建zotero到obsidian的工作流&lt;/h1&gt;
&lt;p&gt;所谓“工欲善其事，必先利其器”&lt;del&gt;（差生文具多）&lt;/del&gt;，想要针对一个领域研究出成果，首先需要建立其对于这个领域完善的知识体系，即形成属于自</description>
        
      
      
      
      <content:encoded><![CDATA[<h1id="从零搭建zotero到obsidian的工作流">从零搭建zotero到obsidian的工作流</h1><p>所谓“工欲善其事，必先利其器”<del>（差生文具多）</del>，想要针对一个领域研究出成果，首先需要建立其对于这个领域完善的知识体系，即形成属于自己的<strong>知识库</strong>。现在有非常多的笔记软件，例如Notion、语雀、Obsidian等等，对于建立知识库非常有帮助，下面我就来谈谈我个人探索出来的一套<strong>从zotero管理文献，到阅读文献做笔记再到obsidian中形成阅读笔记</strong>的丝滑工作流。</p><h2 id="why-zotero">Why Zotero?</h2><p><ahref="https://www.zotero.org/">zotero</a>是一个文献管理软件，据我了解，其前身其实只是一款浏览器插件，用于提供文献元数据的管理，直到后来被开源社区开发成一款跨平台的桌面级软件，并且有非常好的开源社区以及大量的插件。</p><p>文献管理软件当然也不止这一款，至于为什么选择zotero，原因其实非常简单，就是因为它是<strong>开源软件</strong>，我本人是如果功能足够用，一定会优先考虑开源软件的。开源软件一般拥有庞大的专业用户，他们拥有过硬的技术和开源精神，形成的社区非常喜人。</p><p>zotero的基础操作我就不多说了，其实使用起来也比较简单，我主要利用它来分类整理文献，同时搭配ZoteroConnector浏览器插件，在googlescholar中浏览文献的时候直接一键收录并且提取元数据。</p><p>这里推荐两个比较好用的插件，也是我觉得必装的，后续的配置会默认已经安装了这两个插件，插件的配置可以参考网络然后根据自己的需要进行。</p><ul><li><a href="https://retorque.re/zotero-better-bibtex">Better BibTex</a>6.7.19 ——主要用来帮助zotero更好的管理bibliographic数据</li><li><a href="http://zotfile.com/">ztofile</a> 5.1.1 ——更好的管理附件文件，例如格式化命名、移动附件等等</li><li>另外我的Zotero版本是6.0.11</li></ul><p>同时还可以配置Zotero利用云盘进行备份/多设备同步等等，例如我就将我的整个附件目录以及Zotero数据库放在了icloud中。</p><h2 id="why-obsidian">Why Obsidian?</h2><p>说实话，<ahref="https://obsidian.md/">obsidian</a>算是后起之秀了，在知识库软件百花齐放的今天，这个界面不那么友好的软件似乎没有什么竞争力，但对于专业人士来说，恰恰相反。obsidian它所有的特性完美的符合了我的需求。</p><ul><li>文档以markdown格式编写，同时提供扩展语法</li><li>数据存储在本地，且直接以<code>.md</code>格式存储，让人放心</li><li>提供双向链接、关系图谱等功能</li><li>拥有非常庞大的插件支持（例如Vim相关的插件以及等下我们会用到的插件）</li></ul><h2 id="构建文献阅读整理工作流">构建文献阅读整理工作流</h2><p>在我们阅读文献的时候，特别是精读的时候，我觉得比较好的一个流程是：</p><ol type="1"><li>一遍阅读一遍标注一边写评论，这样可以有助于理解文章，同时理清文章的脉络</li><li>利用不同的颜色区分标注的不同内容</li><li>阅读完成后整理阅读笔记，形成知识存储在知识库中，方便以后翻阅，因为通常需要阅读大量的文献，很容易不记得一些文件的细节</li></ol><p>下面我就按照上面的流程来介绍我探索出来的一个工作流</p><h3 id="文献标注">文献标注</h3><p>我会用不同的颜色来表示标注内容的不同意思，通常的习惯是</p><ul><li>橘色表示文章的<strong>主要思想</strong></li><li>黄色表示<strong>研究的细节部分或者系统实现的细节</strong></li><li>红色表示<strong>还需要继续探究的部分或者感兴趣的参考文献</strong></li></ul><p>通过不同颜色的区分不仅有助于我们梳理文章内容，同时也方便接下来导入Obsidian的配置。</p><h3 id="导入obsidian形成笔记">导入Obsidian，形成笔记</h3><p>说了这么多，终于到了重点部分，看起来毫不相关的两个软件，是如何联系起来的呢，这就不得不提到Obsidian的一个插件——<ahref="https://github.com/mgmeyers/obsidian-zotero-integration">zotero-integration</a>了</p><blockquote><p>其实在发现这款插件之前，我是根据<ahref="https://forum.obsidian.md/t/zotero-zotfile-mdnotes-obsidian-dataview-workflow/15536">Zotero-&gt; zotfile -&gt; mdnotes -&gt; obsidian -&gt; dataviewWorkflow</a>这篇帖子在实现我的workflow的，但是后来由于zotero的更新，导致zotero的插件mdnote失效了，所以换成了更稳定的Obsidian端实现</p></blockquote><p>安装好zotero-integration后，可以首先通过<ahref="https://www.bilibili.com/video/BV1jF411A7d6?spm_id_from=333.880.my_history.page.click&amp;vd_source=b31bac4d051d2f11c0776b571298867b">这个视频</a>熟悉它的使用</p><p>现在我们知道了zotero-integration可以通过编写模板实现格式化导入文献笔记</p><p>那么我自己的模板文件如下：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123;title&#125;&#125;</span><br><span class="line">date: &#123;&#123;importDate | format(&quot;YYYY-MM-DD HH:mm:ss&quot;)&#125;&#125;</span><br><span class="line">categories: </span><br><span class="line">tags:&#123;% for tag in tags %&#125;</span><br><span class="line"><span class="section">  - &#123;&#123;tag.tag&#125;&#125;&#123;% endfor %&#125;</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line"></span><br><span class="line"><span class="section">### Matedata</span></span><br><span class="line"><span class="section">#zotero</span></span><br><span class="line"></span><br><span class="line"><span class="section">##### Item Type::</span></span><br><span class="line">[[&#123;&#123;itemType&#125;&#125;]]</span><br><span class="line"></span><br><span class="line"><span class="section">##### Authors::</span></span><br><span class="line">&#123;&#123;authors&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;%if date %&#125;##### Year::</span><br><span class="line">[[&#123;&#123;date | format(&quot;YYYY&quot;)&#125;&#125;]]-&#123;&#123;date | format(&quot;MM&quot;)&#125;&#125;&#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;%if conferenceName %&#125;##### Conference Name</span><br><span class="line">[[&#123;&#123;conferenceName&#125;&#125;]]&#123;% else %&#125;&#123;% if publicationTitle %&#125;##### Publication Title</span><br><span class="line">[[&#123;&#123;publicationTitle&#125;&#125;]]&#123;% endif %&#125;&#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;%if DOI %&#125;##### DOI::</span><br><span class="line">[<span class="string">&#123;&#123;DOI&#125;&#125;</span>](<span class="link">https://doi.org/&#123;&#123;DOI&#125;&#125;</span>)&#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;%if pdfLink %&#125;##### PDF Attachment::</span><br><span class="line">&#123;&#123;pdfLink&#125;&#125;&#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">##### Zotero Link</span></span><br><span class="line"><span class="bullet">-</span> [<span class="string">Local</span>](<span class="link">&#123;&#123;select&#125;&#125;</span>)</span><br><span class="line"><span class="bullet">-</span> [<span class="string">Cloud</span>](<span class="link">&#123;&#123;uri&#125;&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="section"># &#123;&#123;title&#125;&#125;(&#123;%if date %&#125;&#123;&#123;date | format(&quot;YYYY&quot;)&#125;&#125;&#123;% endif %&#125;)</span></span><br><span class="line">&gt;[<span class="string">DOI</span>] [<span class="string">&#123;&#123;DOI&#125;&#125;</span>](<span class="link">https://doi.org/&#123;&#123;DOI&#125;&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="section">## Abstract</span></span><br><span class="line">&#123;&#123;abstractNote&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">## 摘要</span></span><br><span class="line"></span><br><span class="line"><span class="section">## 正文</span></span><br><span class="line">&#123;% for annotation in annotations %&#125;&#123;% if annotation.type == &quot;highlight&quot; %&#125;&#123;% if annotation.colorCategory %&#125;&#123;% if annotation.colorCategory == &quot;Orange&quot; %&#125;</span><br><span class="line"><span class="code">```ad-main_idea</span></span><br><span class="line"><span class="code">&#123;&#123;annotation.annotatedText&#125;&#125;(&#123;% if pdfLink %&#125;&#123;% if citeKey %&#125;[&#123;&#123;citeKey&#125;&#125;]&#123;% elif citationKey %&#125;[&#123;&#123;citationKey&#125;&#125;]&#123;% else %&#125;[Link]&#123;% endif %&#125;&#123;% if annotation.attachment.desktopURI and annotation.page %&#125;(&#123;&#123;annotation.attachment.desktopURI | replace(&quot;select&quot;, &quot;open-pdf&quot;)&#125;&#125;?page=&#123;&#123;annotation.page&#125;&#125;)&#123;% else %&#125;&#123;&#123;pdfLink | replace(r/^\[.*\]/, &quot;&quot;)&#125;&#125;&#123;% endif %&#125;&#123;% endif %&#125;)</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line">&#123;% elif annotation.colorCategory == &quot;Yellow&quot; %&#125;</span><br><span class="line"><span class="code">```ad-detail</span></span><br><span class="line"><span class="code">&#123;&#123;annotation.annotatedText&#125;&#125;(&#123;% if pdfLink %&#125;&#123;% if citeKey %&#125;[&#123;&#123;citeKey&#125;&#125;]&#123;% elif citationKey %&#125;[&#123;&#123;citationKey&#125;&#125;]&#123;% else %&#125;[Link]&#123;% endif %&#125;&#123;% if annotation.attachment.desktopURI and annotation.page %&#125;(&#123;&#123;annotation.attachment.desktopURI | replace(&quot;select&quot;, &quot;open-pdf&quot;)&#125;&#125;?page=&#123;&#123;annotation.page&#125;&#125;)&#123;% else %&#125;&#123;&#123;pdfLink | replace(r/^\[.*\]/, &quot;&quot;)&#125;&#125;&#123;% endif %&#125;&#123;% endif %&#125;)</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line">&#123;% elif annotation.colorCategory == &quot;Red&quot; %&#125;</span><br><span class="line"><span class="code">```ad-todo</span></span><br><span class="line"><span class="code">- [ ] &#123;&#123;annotation.annotatedText&#125;&#125;(&#123;% if pdfLink %&#125;&#123;% if citeKey %&#125;[&#123;&#123;citeKey&#125;&#125;]&#123;% elif citationKey %&#125;[&#123;&#123;citationKey&#125;&#125;]&#123;% else %&#125;[Link]&#123;% endif %&#125;&#123;% if annotation.attachment.desktopURI and annotation.page %&#125;(&#123;&#123;annotation.attachment.desktopURI | replace(&quot;select&quot;, &quot;open-pdf&quot;)&#125;&#125;?page=&#123;&#123;annotation.page&#125;&#125;)&#123;% else %&#125;&#123;&#123;pdfLink | replace(r/^\[.*\]/, &quot;&quot;)&#125;&#125;&#123;% endif %&#125;&#123;% endif %&#125;)</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line">&#123;% endif %&#125;&#123;% endif %&#125;&#123;% if annotation.comment %&#125;&#123;&#123;annotation.comment&#125;&#125;&#123;% endif %&#125;&#123;% endif %&#125;&#123;% endfor %&#125;</span><br></pre></td></tr></table></figure><p>这个模板有几个比较有意思的地方</p><ol type="1"><li>元数据单独管理，可以方便迁移，本地链接全部存在元数据中。正文中不存在本地链接，直接复制到别的地方也可以正确的跳转</li><li>它的front-matter部分我直接按照Hexo博客模板给出，这样不仅可以更加方便的管理元数据，还可以让我直接复制粘贴到Hexo当作博文发布</li><li>搭配<ahref="https://github.com/valentine195/obsidian-admonition">admonition</a>使用可以实现贴纸效果，笔记一目了然，当然如果不喜欢也可以简化<code>&#123;% for annotation in annotations %&#125;</code>里面的实现</li></ol><p>下面给一些导入之后的效果，每次阅读完文献、整理之后，就形成自己的知识。</p>]]></content:encoded>
      
      
      <category domain="https://jiengup.github.io/categories/Tools/">Tools</category>
      
      
      <category domain="https://jiengup.github.io/tags/tool/">tool</category>
      
      <category domain="https://jiengup.github.io/tags/research/">research</category>
      
      
      <comments>https://jiengup.github.io/2022/08/10/19/55/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
